{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a591a46e",
   "metadata": {},
   "source": [
    "- 다중 control signal을 사용하는 model을 만들기 위한 곳\n",
    "- 현재 분석으로는 256 by 256으로 finetune 되었다는 점이 크게 작용한다.\n",
    "- 약간의 variant라고 한다면, \"model.diffusion_model.input_blocks.0.0.weight\" 이 부분이 input dimension이 8이 되었기 떄문에 avearage를 취해야 겠다.\n",
    "- 학습 scale이 lab scale은 아니고 중소기업 정도는 되기 때문에, 역시 이를 control signal형태로 사용하는 것은 적절한 관측이 될 것으로 보인다.\n",
    "- 그래도 30K scale의 finetune을 거치기 때문에, spatial resolution에 의한 문제는 크게 발생하지 않을 것으로 보인다.\n",
    "- [ ] control signal composition에 대한 실험만 거치면 될 것으로 보인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82628974",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e789bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import copy\n",
    "\n",
    "repo_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(repo_path)\n",
    "\n",
    "repo_path_2 = \"/workspace/code/3DAnything/zero123\"\n",
    "sys.path.append(repo_path_2)\n",
    "\n",
    "from zero123.ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9f8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_dict_to_jsonl(state_dict, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        for key, value in state_dict.items():\n",
    "            json_line = {\"key\": key, \"shape\": value.shape}\n",
    "            json.dump(json_line, f, ensure_ascii=False)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfdf626",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg_path = \"/workspace/code/3DAnything/zero123/configs/canny-edge.yaml\"\n",
    "test_cfg = OmegaConf.load(test_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46066032",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig_cfg_path = \"/workspace/code/3DAnything/zero123/configs/sd-objaverse-finetune-c_concat-256.yaml\"\n",
    "test_orig_cfg = OmegaConf.load(test_orig_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/clip/clip.py:57: UserWarning: /root/.cache/clip/ViT-L-14.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "100%|████████████████████████████████████████| 890M/890M [20:48<00:00, 747kiB/s]\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_from_config(test_cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95b5df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.53 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'diffusion_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_orig \u001b[38;5;241m=\u001b[39m instantiate_from_config(test_orig_cfg\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdiffusion_model\u001b[49m\u001b[38;5;241m.\u001b[39minput_blocks\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diffusion_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_orig = instantiate_from_config(test_orig_cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e2b41c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDiffusion(\n",
       "  (model): DiffusionWrapper(\n",
       "    (diffusion_model): UNetModel(\n",
       "      (time_embed): Sequential(\n",
       "        (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (input_blocks): ModuleList(\n",
       "        (0): TimestepEmbedSequential(\n",
       "          (0): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (6): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (9): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10-11): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (middle_block): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_blocks): ModuleList(\n",
       "        (0-1): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Upsample(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3-4): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (6): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (9): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10-11): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out): Sequential(\n",
       "        (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model_ema): LitEma()\n",
       "  (first_stage_model): AutoencoderKL(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "      )\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (up): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2-3): 2 x Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (loss): Identity()\n",
       "    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (cond_stage_model): FrozenCLIPImageEmbedder(\n",
       "    (model): CLIP(\n",
       "      (visual): VisionTransformer(\n",
       "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer): Transformer(\n",
       "          (resblocks): Sequential(\n",
       "            (0): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (12): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (13): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (14): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (15): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (16): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (17): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (18): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (19): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (20): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (21): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (22): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (23): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cc_projection): Linear(in_features=772, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84fb6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_state_dict_to_jsonl(model_orig.state_dict(), \"model_orig_state_dict.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a451973",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig_weight = torch.load(\"/workspace/weight/diffuse/zero123-weights/165000.ckpt\", map_location=\"cpu\")[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4aae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT\n",
    "new_zero123_checkpoint = {} \n",
    "for k, v in model_orgi_stdict.items():\n",
    "    if (\"diffusion_model\" in k and \"model_ema\" not in k) and (\"input_blocks\" in k or \"middle_block\" in k):\n",
    "        _k = k.replace(\"model.diffusion_model.\", \"\")\n",
    "        \n",
    "        if \"input_blocks.0.0.weight\" in _k:\n",
    "            # (320, 8, 3, 3) -> (320, 4, 2, 3, 3) -> (320, 4, 3, 3)\n",
    "            v_shape = v.shape\n",
    "            v = v.view(v_shape[0], v_shape[1] /2, 2, v_shape[2], v_shape[3])\n",
    "            v = v.mean(dim=2)\n",
    "        \n",
    "        if \"zero_convs\" in _k:\n",
    "            v = torch.zeros_like(v)\n",
    "            \n",
    "        new_zero123_checkpoint[_k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86da5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_blocks.0.0.weight', 'input_blocks.0.0.bias', 'input_blocks.1.0.in_layers.0.weight', 'input_blocks.1.0.in_layers.0.bias', 'input_blocks.1.0.in_layers.2.weight', 'input_blocks.1.0.in_layers.2.bias', 'input_blocks.1.0.emb_layers.1.weight', 'input_blocks.1.0.emb_layers.1.bias', 'input_blocks.1.0.out_layers.0.weight', 'input_blocks.1.0.out_layers.0.bias', 'input_blocks.1.0.out_layers.3.weight', 'input_blocks.1.0.out_layers.3.bias', 'input_blocks.1.1.norm.weight', 'input_blocks.1.1.norm.bias', 'input_blocks.1.1.proj_in.weight', 'input_blocks.1.1.proj_in.bias', 'input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.1.1.transformer_blocks.0.norm1.weight', 'input_blocks.1.1.transformer_blocks.0.norm1.bias', 'input_blocks.1.1.transformer_blocks.0.norm2.weight', 'input_blocks.1.1.transformer_blocks.0.norm2.bias', 'input_blocks.1.1.transformer_blocks.0.norm3.weight', 'input_blocks.1.1.transformer_blocks.0.norm3.bias', 'input_blocks.1.1.proj_out.weight', 'input_blocks.1.1.proj_out.bias', 'input_blocks.2.0.in_layers.0.weight', 'input_blocks.2.0.in_layers.0.bias', 'input_blocks.2.0.in_layers.2.weight', 'input_blocks.2.0.in_layers.2.bias', 'input_blocks.2.0.emb_layers.1.weight', 'input_blocks.2.0.emb_layers.1.bias', 'input_blocks.2.0.out_layers.0.weight', 'input_blocks.2.0.out_layers.0.bias', 'input_blocks.2.0.out_layers.3.weight', 'input_blocks.2.0.out_layers.3.bias', 'input_blocks.2.1.norm.weight', 'input_blocks.2.1.norm.bias', 'input_blocks.2.1.proj_in.weight', 'input_blocks.2.1.proj_in.bias', 'input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.2.1.transformer_blocks.0.norm1.weight', 'input_blocks.2.1.transformer_blocks.0.norm1.bias', 'input_blocks.2.1.transformer_blocks.0.norm2.weight', 'input_blocks.2.1.transformer_blocks.0.norm2.bias', 'input_blocks.2.1.transformer_blocks.0.norm3.weight', 'input_blocks.2.1.transformer_blocks.0.norm3.bias', 'input_blocks.2.1.proj_out.weight', 'input_blocks.2.1.proj_out.bias', 'input_blocks.3.0.op.weight', 'input_blocks.3.0.op.bias', 'input_blocks.4.0.in_layers.0.weight', 'input_blocks.4.0.in_layers.0.bias', 'input_blocks.4.0.in_layers.2.weight', 'input_blocks.4.0.in_layers.2.bias', 'input_blocks.4.0.emb_layers.1.weight', 'input_blocks.4.0.emb_layers.1.bias', 'input_blocks.4.0.out_layers.0.weight', 'input_blocks.4.0.out_layers.0.bias', 'input_blocks.4.0.out_layers.3.weight', 'input_blocks.4.0.out_layers.3.bias', 'input_blocks.4.0.skip_connection.weight', 'input_blocks.4.0.skip_connection.bias', 'input_blocks.4.1.norm.weight', 'input_blocks.4.1.norm.bias', 'input_blocks.4.1.proj_in.weight', 'input_blocks.4.1.proj_in.bias', 'input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.4.1.transformer_blocks.0.norm1.weight', 'input_blocks.4.1.transformer_blocks.0.norm1.bias', 'input_blocks.4.1.transformer_blocks.0.norm2.weight', 'input_blocks.4.1.transformer_blocks.0.norm2.bias', 'input_blocks.4.1.transformer_blocks.0.norm3.weight', 'input_blocks.4.1.transformer_blocks.0.norm3.bias', 'input_blocks.4.1.proj_out.weight', 'input_blocks.4.1.proj_out.bias', 'input_blocks.5.0.in_layers.0.weight', 'input_blocks.5.0.in_layers.0.bias', 'input_blocks.5.0.in_layers.2.weight', 'input_blocks.5.0.in_layers.2.bias', 'input_blocks.5.0.emb_layers.1.weight', 'input_blocks.5.0.emb_layers.1.bias', 'input_blocks.5.0.out_layers.0.weight', 'input_blocks.5.0.out_layers.0.bias', 'input_blocks.5.0.out_layers.3.weight', 'input_blocks.5.0.out_layers.3.bias', 'input_blocks.5.1.norm.weight', 'input_blocks.5.1.norm.bias', 'input_blocks.5.1.proj_in.weight', 'input_blocks.5.1.proj_in.bias', 'input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.5.1.transformer_blocks.0.norm1.weight', 'input_blocks.5.1.transformer_blocks.0.norm1.bias', 'input_blocks.5.1.transformer_blocks.0.norm2.weight', 'input_blocks.5.1.transformer_blocks.0.norm2.bias', 'input_blocks.5.1.transformer_blocks.0.norm3.weight', 'input_blocks.5.1.transformer_blocks.0.norm3.bias', 'input_blocks.5.1.proj_out.weight', 'input_blocks.5.1.proj_out.bias', 'input_blocks.6.0.op.weight', 'input_blocks.6.0.op.bias', 'input_blocks.7.0.in_layers.0.weight', 'input_blocks.7.0.in_layers.0.bias', 'input_blocks.7.0.in_layers.2.weight', 'input_blocks.7.0.in_layers.2.bias', 'input_blocks.7.0.emb_layers.1.weight', 'input_blocks.7.0.emb_layers.1.bias', 'input_blocks.7.0.out_layers.0.weight', 'input_blocks.7.0.out_layers.0.bias', 'input_blocks.7.0.out_layers.3.weight', 'input_blocks.7.0.out_layers.3.bias', 'input_blocks.7.0.skip_connection.weight', 'input_blocks.7.0.skip_connection.bias', 'input_blocks.7.1.norm.weight', 'input_blocks.7.1.norm.bias', 'input_blocks.7.1.proj_in.weight', 'input_blocks.7.1.proj_in.bias', 'input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.7.1.transformer_blocks.0.norm1.weight', 'input_blocks.7.1.transformer_blocks.0.norm1.bias', 'input_blocks.7.1.transformer_blocks.0.norm2.weight', 'input_blocks.7.1.transformer_blocks.0.norm2.bias', 'input_blocks.7.1.transformer_blocks.0.norm3.weight', 'input_blocks.7.1.transformer_blocks.0.norm3.bias', 'input_blocks.7.1.proj_out.weight', 'input_blocks.7.1.proj_out.bias', 'input_blocks.8.0.in_layers.0.weight', 'input_blocks.8.0.in_layers.0.bias', 'input_blocks.8.0.in_layers.2.weight', 'input_blocks.8.0.in_layers.2.bias', 'input_blocks.8.0.emb_layers.1.weight', 'input_blocks.8.0.emb_layers.1.bias', 'input_blocks.8.0.out_layers.0.weight', 'input_blocks.8.0.out_layers.0.bias', 'input_blocks.8.0.out_layers.3.weight', 'input_blocks.8.0.out_layers.3.bias', 'input_blocks.8.1.norm.weight', 'input_blocks.8.1.norm.bias', 'input_blocks.8.1.proj_in.weight', 'input_blocks.8.1.proj_in.bias', 'input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.8.1.transformer_blocks.0.norm1.weight', 'input_blocks.8.1.transformer_blocks.0.norm1.bias', 'input_blocks.8.1.transformer_blocks.0.norm2.weight', 'input_blocks.8.1.transformer_blocks.0.norm2.bias', 'input_blocks.8.1.transformer_blocks.0.norm3.weight', 'input_blocks.8.1.transformer_blocks.0.norm3.bias', 'input_blocks.8.1.proj_out.weight', 'input_blocks.8.1.proj_out.bias', 'input_blocks.9.0.op.weight', 'input_blocks.9.0.op.bias', 'input_blocks.10.0.in_layers.0.weight', 'input_blocks.10.0.in_layers.0.bias', 'input_blocks.10.0.in_layers.2.weight', 'input_blocks.10.0.in_layers.2.bias', 'input_blocks.10.0.emb_layers.1.weight', 'input_blocks.10.0.emb_layers.1.bias', 'input_blocks.10.0.out_layers.0.weight', 'input_blocks.10.0.out_layers.0.bias', 'input_blocks.10.0.out_layers.3.weight', 'input_blocks.10.0.out_layers.3.bias', 'input_blocks.11.0.in_layers.0.weight', 'input_blocks.11.0.in_layers.0.bias', 'input_blocks.11.0.in_layers.2.weight', 'input_blocks.11.0.in_layers.2.bias', 'input_blocks.11.0.emb_layers.1.weight', 'input_blocks.11.0.emb_layers.1.bias', 'input_blocks.11.0.out_layers.0.weight', 'input_blocks.11.0.out_layers.0.bias', 'input_blocks.11.0.out_layers.3.weight', 'input_blocks.11.0.out_layers.3.bias', 'middle_block.0.in_layers.0.weight', 'middle_block.0.in_layers.0.bias', 'middle_block.0.in_layers.2.weight', 'middle_block.0.in_layers.2.bias', 'middle_block.0.emb_layers.1.weight', 'middle_block.0.emb_layers.1.bias', 'middle_block.0.out_layers.0.weight', 'middle_block.0.out_layers.0.bias', 'middle_block.0.out_layers.3.weight', 'middle_block.0.out_layers.3.bias', 'middle_block.1.norm.weight', 'middle_block.1.norm.bias', 'middle_block.1.proj_in.weight', 'middle_block.1.proj_in.bias', 'middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'middle_block.1.transformer_blocks.0.ff.net.2.weight', 'middle_block.1.transformer_blocks.0.ff.net.2.bias', 'middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'middle_block.1.transformer_blocks.0.norm1.weight', 'middle_block.1.transformer_blocks.0.norm1.bias', 'middle_block.1.transformer_blocks.0.norm2.weight', 'middle_block.1.transformer_blocks.0.norm2.bias', 'middle_block.1.transformer_blocks.0.norm3.weight', 'middle_block.1.transformer_blocks.0.norm3.bias', 'middle_block.1.proj_out.weight', 'middle_block.1.proj_out.bias', 'middle_block.2.in_layers.0.weight', 'middle_block.2.in_layers.0.bias', 'middle_block.2.in_layers.2.weight', 'middle_block.2.in_layers.2.bias', 'middle_block.2.emb_layers.1.weight', 'middle_block.2.emb_layers.1.bias', 'middle_block.2.out_layers.0.weight', 'middle_block.2.out_layers.0.bias', 'middle_block.2.out_layers.3.weight', 'middle_block.2.out_layers.3.bias'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_zero123_checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719432c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weight = copy.copy(new_zero123_checkpoint[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55bfb4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 4, 3, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weight.reshape(320, 4, 2, 3, 3).mean(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bff5faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(1, 3, 256, 256)\n",
    "test_tensor = test_tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.first_stage_model\n",
    "vae = model.first_stage_model.to(\"cuda\")\n",
    "# 여기는 그대로 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ad5e751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_2 = model_orig.first_stage_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a18557a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vae.encode(test_tensor)\n",
    "result_2 = vae_2.encode(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b2db923",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sampled = result.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e644865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32, 32])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "929a0943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32, 32])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.mean.shape\n",
    "result_2.mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddc5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ControlledUnetModel(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0-1): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3-4): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.diffusion_model\n",
    "# 여기를 T2I 모델로 바꿔야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ebda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): TimestepEmbedSequential(\n",
       "    (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (1-2): 2 x TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (3): TimestepEmbedSequential(\n",
       "    (0): Downsample(\n",
       "      (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (4): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (5): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (6): TimestepEmbedSequential(\n",
       "    (0): Downsample(\n",
       "      (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (7): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (8): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (9): TimestepEmbedSequential(\n",
       "    (0): Downsample(\n",
       "      (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (10-11): 2 x TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.diffusion_model.input_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663db600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenCLIPImageEmbedder(\n",
       "  (model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cond_stage_model\n",
    "# 이 부분의 clip을 치환하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92ac5200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-3): 4 x TimestepEmbedSequential(\n",
       "    (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (4-6): 3 x TimestepEmbedSequential(\n",
       "    (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (7-11): 5 x TimestepEmbedSequential(\n",
       "    (0): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.control_model.zero_convs\n",
    "# 여기는 code implementation을 약간 바꿔야 한다. 이종 control을 둘 다 넣기 위해서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568bc26",
   "metadata": {},
   "source": [
    "# test_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f2427e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (3): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (loss): Identity()\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig.first_stage_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19bfe623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0-1): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3-4): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8226c3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 8, 3, 3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig.model.diffusion_model.input_blocks[0][0].weight.shape\n",
    "# 여기 convolution layer만 control하면 될 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e99be",
   "metadata": {},
   "source": [
    "# SD weight 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f6b46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_path = \"/workspace/weight/diffuse/stable-diffusion-v-1-4-original/sd-v1-4.ckpt\"\n",
    "sd_cfg_path = \"/workspace/code/3DAnything/zero123/configs/sd-v1-inference.yaml\"\n",
    "sd_cfg = OmegaConf.load(sd_cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1bbbd73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'base_learning_rate': 0.0001, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'jpg', 'cond_stage_key': 'txt', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scheduler_config': {'target': 'ldm.lr_scheduler.LambdaLinearScheduler', 'params': {'warm_up_steps': [10000], 'cycle_lengths': [10000000000000], 'f_start': [1e-06], 'f_max': [1.0], 'f_min': [1.0]}}, 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.FrozenCLIPEmbedder'}}}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0bbe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "sd_model = instantiate_from_config(sd_cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1aa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weight = torch.load(sd_path, map_location=\"cpu\")[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ab8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['betas', 'alphas_cumprod', 'alphas_cumprod_prev', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'log_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod', 'posterior_variance', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias', 'model_ema.decay', 'model_ema.num_updates', 'first_stage_model.encoder.conv_in.weight', 'first_stage_model.encoder.conv_in.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.weight', 'first_stage_model.encoder.down.0.block.0.conv1.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.weight', 'first_stage_model.encoder.down.0.block.0.conv2.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.weight', 'first_stage_model.encoder.down.0.block.1.conv1.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.weight', 'first_stage_model.encoder.down.0.block.1.conv2.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.weight', 'first_stage_model.encoder.down.1.block.0.conv1.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.weight', 'first_stage_model.encoder.down.1.block.0.conv2.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.weight', 'first_stage_model.encoder.down.1.block.1.conv1.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.weight', 'first_stage_model.encoder.down.1.block.1.conv2.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.weight', 'first_stage_model.encoder.down.2.block.0.conv1.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.weight', 'first_stage_model.encoder.down.2.block.0.conv2.bias', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.weight', 'first_stage_model.encoder.down.2.block.1.conv1.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.weight', 'first_stage_model.encoder.down.2.block.1.conv2.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.weight', 'first_stage_model.encoder.down.3.block.0.conv1.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.weight', 'first_stage_model.encoder.down.3.block.0.conv2.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.weight', 'first_stage_model.encoder.down.3.block.1.conv1.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.weight', 'first_stage_model.encoder.down.3.block.1.conv2.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.weight', 'first_stage_model.encoder.mid.block_1.conv1.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.weight', 'first_stage_model.encoder.mid.block_1.conv2.bias', 'first_stage_model.encoder.mid.attn_1.norm.weight', 'first_stage_model.encoder.mid.attn_1.norm.bias', 'first_stage_model.encoder.mid.attn_1.q.weight', 'first_stage_model.encoder.mid.attn_1.q.bias', 'first_stage_model.encoder.mid.attn_1.k.weight', 'first_stage_model.encoder.mid.attn_1.k.bias', 'first_stage_model.encoder.mid.attn_1.v.weight', 'first_stage_model.encoder.mid.attn_1.v.bias', 'first_stage_model.encoder.mid.attn_1.proj_out.weight', 'first_stage_model.encoder.mid.attn_1.proj_out.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.weight', 'first_stage_model.encoder.mid.block_2.conv1.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.weight', 'first_stage_model.encoder.mid.block_2.conv2.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.weight', 'first_stage_model.encoder.conv_out.bias', 'first_stage_model.decoder.conv_in.weight', 'first_stage_model.decoder.conv_in.bias', 'first_stage_model.decoder.mid.block_1.norm1.weight', 'first_stage_model.decoder.mid.block_1.norm1.bias', 'first_stage_model.decoder.mid.block_1.conv1.weight', 'first_stage_model.decoder.mid.block_1.conv1.bias', 'first_stage_model.decoder.mid.block_1.norm2.weight', 'first_stage_model.decoder.mid.block_1.norm2.bias', 'first_stage_model.decoder.mid.block_1.conv2.weight', 'first_stage_model.decoder.mid.block_1.conv2.bias', 'first_stage_model.decoder.mid.attn_1.norm.weight', 'first_stage_model.decoder.mid.attn_1.norm.bias', 'first_stage_model.decoder.mid.attn_1.q.weight', 'first_stage_model.decoder.mid.attn_1.q.bias', 'first_stage_model.decoder.mid.attn_1.k.weight', 'first_stage_model.decoder.mid.attn_1.k.bias', 'first_stage_model.decoder.mid.attn_1.v.weight', 'first_stage_model.decoder.mid.attn_1.v.bias', 'first_stage_model.decoder.mid.attn_1.proj_out.weight', 'first_stage_model.decoder.mid.attn_1.proj_out.bias', 'first_stage_model.decoder.mid.block_2.norm1.weight', 'first_stage_model.decoder.mid.block_2.norm1.bias', 'first_stage_model.decoder.mid.block_2.conv1.weight', 'first_stage_model.decoder.mid.block_2.conv1.bias', 'first_stage_model.decoder.mid.block_2.norm2.weight', 'first_stage_model.decoder.mid.block_2.norm2.bias', 'first_stage_model.decoder.mid.block_2.conv2.weight', 'first_stage_model.decoder.mid.block_2.conv2.bias', 'first_stage_model.decoder.up.0.block.0.norm1.weight', 'first_stage_model.decoder.up.0.block.0.norm1.bias', 'first_stage_model.decoder.up.0.block.0.conv1.weight', 'first_stage_model.decoder.up.0.block.0.conv1.bias', 'first_stage_model.decoder.up.0.block.0.norm2.weight', 'first_stage_model.decoder.up.0.block.0.norm2.bias', 'first_stage_model.decoder.up.0.block.0.conv2.weight', 'first_stage_model.decoder.up.0.block.0.conv2.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.weight', 'first_stage_model.decoder.up.0.block.1.norm1.bias', 'first_stage_model.decoder.up.0.block.1.conv1.weight', 'first_stage_model.decoder.up.0.block.1.conv1.bias', 'first_stage_model.decoder.up.0.block.1.norm2.weight', 'first_stage_model.decoder.up.0.block.1.norm2.bias', 'first_stage_model.decoder.up.0.block.1.conv2.weight', 'first_stage_model.decoder.up.0.block.1.conv2.bias', 'first_stage_model.decoder.up.0.block.2.norm1.weight', 'first_stage_model.decoder.up.0.block.2.norm1.bias', 'first_stage_model.decoder.up.0.block.2.conv1.weight', 'first_stage_model.decoder.up.0.block.2.conv1.bias', 'first_stage_model.decoder.up.0.block.2.norm2.weight', 'first_stage_model.decoder.up.0.block.2.norm2.bias', 'first_stage_model.decoder.up.0.block.2.conv2.weight', 'first_stage_model.decoder.up.0.block.2.conv2.bias', 'first_stage_model.decoder.up.1.block.0.norm1.weight', 'first_stage_model.decoder.up.1.block.0.norm1.bias', 'first_stage_model.decoder.up.1.block.0.conv1.weight', 'first_stage_model.decoder.up.1.block.0.conv1.bias', 'first_stage_model.decoder.up.1.block.0.norm2.weight', 'first_stage_model.decoder.up.1.block.0.norm2.bias', 'first_stage_model.decoder.up.1.block.0.conv2.weight', 'first_stage_model.decoder.up.1.block.0.conv2.bias', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.1.block.1.norm1.weight', 'first_stage_model.decoder.up.1.block.1.norm1.bias', 'first_stage_model.decoder.up.1.block.1.conv1.weight', 'first_stage_model.decoder.up.1.block.1.conv1.bias', 'first_stage_model.decoder.up.1.block.1.norm2.weight', 'first_stage_model.decoder.up.1.block.1.norm2.bias', 'first_stage_model.decoder.up.1.block.1.conv2.weight', 'first_stage_model.decoder.up.1.block.1.conv2.bias', 'first_stage_model.decoder.up.1.block.2.norm1.weight', 'first_stage_model.decoder.up.1.block.2.norm1.bias', 'first_stage_model.decoder.up.1.block.2.conv1.weight', 'first_stage_model.decoder.up.1.block.2.conv1.bias', 'first_stage_model.decoder.up.1.block.2.norm2.weight', 'first_stage_model.decoder.up.1.block.2.norm2.bias', 'first_stage_model.decoder.up.1.block.2.conv2.weight', 'first_stage_model.decoder.up.1.block.2.conv2.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.weight', 'first_stage_model.decoder.up.2.block.0.norm1.bias', 'first_stage_model.decoder.up.2.block.0.conv1.weight', 'first_stage_model.decoder.up.2.block.0.conv1.bias', 'first_stage_model.decoder.up.2.block.0.norm2.weight', 'first_stage_model.decoder.up.2.block.0.norm2.bias', 'first_stage_model.decoder.up.2.block.0.conv2.weight', 'first_stage_model.decoder.up.2.block.0.conv2.bias', 'first_stage_model.decoder.up.2.block.1.norm1.weight', 'first_stage_model.decoder.up.2.block.1.norm1.bias', 'first_stage_model.decoder.up.2.block.1.conv1.weight', 'first_stage_model.decoder.up.2.block.1.conv1.bias', 'first_stage_model.decoder.up.2.block.1.norm2.weight', 'first_stage_model.decoder.up.2.block.1.norm2.bias', 'first_stage_model.decoder.up.2.block.1.conv2.weight', 'first_stage_model.decoder.up.2.block.1.conv2.bias', 'first_stage_model.decoder.up.2.block.2.norm1.weight', 'first_stage_model.decoder.up.2.block.2.norm1.bias', 'first_stage_model.decoder.up.2.block.2.conv1.weight', 'first_stage_model.decoder.up.2.block.2.conv1.bias', 'first_stage_model.decoder.up.2.block.2.norm2.weight', 'first_stage_model.decoder.up.2.block.2.norm2.bias', 'first_stage_model.decoder.up.2.block.2.conv2.weight', 'first_stage_model.decoder.up.2.block.2.conv2.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.weight', 'first_stage_model.decoder.up.3.block.0.norm1.bias', 'first_stage_model.decoder.up.3.block.0.conv1.weight', 'first_stage_model.decoder.up.3.block.0.conv1.bias', 'first_stage_model.decoder.up.3.block.0.norm2.weight', 'first_stage_model.decoder.up.3.block.0.norm2.bias', 'first_stage_model.decoder.up.3.block.0.conv2.weight', 'first_stage_model.decoder.up.3.block.0.conv2.bias', 'first_stage_model.decoder.up.3.block.1.norm1.weight', 'first_stage_model.decoder.up.3.block.1.norm1.bias', 'first_stage_model.decoder.up.3.block.1.conv1.weight', 'first_stage_model.decoder.up.3.block.1.conv1.bias', 'first_stage_model.decoder.up.3.block.1.norm2.weight', 'first_stage_model.decoder.up.3.block.1.norm2.bias', 'first_stage_model.decoder.up.3.block.1.conv2.weight', 'first_stage_model.decoder.up.3.block.1.conv2.bias', 'first_stage_model.decoder.up.3.block.2.norm1.weight', 'first_stage_model.decoder.up.3.block.2.norm1.bias', 'first_stage_model.decoder.up.3.block.2.conv1.weight', 'first_stage_model.decoder.up.3.block.2.conv1.bias', 'first_stage_model.decoder.up.3.block.2.norm2.weight', 'first_stage_model.decoder.up.3.block.2.norm2.bias', 'first_stage_model.decoder.up.3.block.2.conv2.weight', 'first_stage_model.decoder.up.3.block.2.conv2.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.weight', 'first_stage_model.decoder.norm_out.bias', 'first_stage_model.decoder.conv_out.weight', 'first_stage_model.decoder.conv_out.bias', 'first_stage_model.quant_conv.weight', 'first_stage_model.quant_conv.bias', 'first_stage_model.post_quant_conv.weight', 'first_stage_model.post_quant_conv.bias', 'cond_stage_model.transformer.text_model.embeddings.position_ids', 'cond_stage_model.transformer.text_model.embeddings.token_embedding.weight', 'cond_stage_model.transformer.text_model.embeddings.position_embedding.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias', 'cond_stage_model.transformer.text_model.final_layer_norm.weight', 'cond_stage_model.transformer.text_model.final_layer_norm.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(test_weight.keys())\n",
    "convert_state_dict_to_jsonl(test_weight, \"sd_v1_4_weights.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b4a82419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 4, 3, 3])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_model.model.diffusion_model.input_blocks[0][0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55d6d2a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sd_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msd_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(test_weight, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sd_model' is not defined"
     ]
    }
   ],
   "source": [
    "sd_model.load_state_dict(test_weight, strict=False)\n",
    "# 'cond_stage_model.transformer.text_model.embeddings.position_ids' 이것 말고는 적절하게 load된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87daf9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDiffusion(\n",
       "  (model): DiffusionWrapper(\n",
       "    (diffusion_model): UNetModel(\n",
       "      (time_embed): Sequential(\n",
       "        (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (input_blocks): ModuleList(\n",
       "        (0): TimestepEmbedSequential(\n",
       "          (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (6): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (9): TimestepEmbedSequential(\n",
       "          (0): Downsample(\n",
       "            (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10-11): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (middle_block): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_blocks): ModuleList(\n",
       "        (0-1): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Upsample(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3-4): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (6): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Upsample(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (9): TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10-11): 2 x TimestepEmbedSequential(\n",
       "          (0): ResBlock(\n",
       "            (in_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (h_upd): Identity()\n",
       "            (x_upd): Identity()\n",
       "            (emb_layers): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "            (out_layers): Sequential(\n",
       "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "              (1): SiLU()\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): SpatialTransformer(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (attn1): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (attn2): CrossAttention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out): Sequential(\n",
       "        (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (first_stage_model): AutoencoderKL(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "      )\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (up): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2-3): 2 x Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (loss): Identity()\n",
       "    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (cond_stage_model): FrozenCLIPEmbedder(\n",
       "    (transformer): CLIPTextModel(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 768)\n",
       "          (position_embedding): Embedding(77, 768)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cc_projection): Linear(in_features=772, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcaa8f6",
   "metadata": {},
   "source": [
    "# ControlNet wieight 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_canny_path = os.path.join(\"/workspace/weight/diffuse/ControlNet/models\" ,\"control_sd15_canny.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b580f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_net_ckpt = torch.load(control_canny_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "125106f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['betas', 'alphas_cumprod', 'alphas_cumprod_prev', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'log_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod', 'posterior_variance', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'logvar', 'model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias', 'first_stage_model.encoder.conv_in.weight', 'first_stage_model.encoder.conv_in.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.weight', 'first_stage_model.encoder.down.0.block.0.conv1.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.weight', 'first_stage_model.encoder.down.0.block.0.conv2.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.weight', 'first_stage_model.encoder.down.0.block.1.conv1.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.weight', 'first_stage_model.encoder.down.0.block.1.conv2.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.weight', 'first_stage_model.encoder.down.1.block.0.conv1.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.weight', 'first_stage_model.encoder.down.1.block.0.conv2.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.weight', 'first_stage_model.encoder.down.1.block.1.conv1.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.weight', 'first_stage_model.encoder.down.1.block.1.conv2.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.weight', 'first_stage_model.encoder.down.2.block.0.conv1.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.weight', 'first_stage_model.encoder.down.2.block.0.conv2.bias', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.weight', 'first_stage_model.encoder.down.2.block.1.conv1.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.weight', 'first_stage_model.encoder.down.2.block.1.conv2.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.weight', 'first_stage_model.encoder.down.3.block.0.conv1.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.weight', 'first_stage_model.encoder.down.3.block.0.conv2.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.weight', 'first_stage_model.encoder.down.3.block.1.conv1.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.weight', 'first_stage_model.encoder.down.3.block.1.conv2.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.weight', 'first_stage_model.encoder.mid.block_1.conv1.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.weight', 'first_stage_model.encoder.mid.block_1.conv2.bias', 'first_stage_model.encoder.mid.attn_1.norm.weight', 'first_stage_model.encoder.mid.attn_1.norm.bias', 'first_stage_model.encoder.mid.attn_1.q.weight', 'first_stage_model.encoder.mid.attn_1.q.bias', 'first_stage_model.encoder.mid.attn_1.k.weight', 'first_stage_model.encoder.mid.attn_1.k.bias', 'first_stage_model.encoder.mid.attn_1.v.weight', 'first_stage_model.encoder.mid.attn_1.v.bias', 'first_stage_model.encoder.mid.attn_1.proj_out.weight', 'first_stage_model.encoder.mid.attn_1.proj_out.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.weight', 'first_stage_model.encoder.mid.block_2.conv1.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.weight', 'first_stage_model.encoder.mid.block_2.conv2.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.weight', 'first_stage_model.encoder.conv_out.bias', 'first_stage_model.decoder.conv_in.weight', 'first_stage_model.decoder.conv_in.bias', 'first_stage_model.decoder.mid.block_1.norm1.weight', 'first_stage_model.decoder.mid.block_1.norm1.bias', 'first_stage_model.decoder.mid.block_1.conv1.weight', 'first_stage_model.decoder.mid.block_1.conv1.bias', 'first_stage_model.decoder.mid.block_1.norm2.weight', 'first_stage_model.decoder.mid.block_1.norm2.bias', 'first_stage_model.decoder.mid.block_1.conv2.weight', 'first_stage_model.decoder.mid.block_1.conv2.bias', 'first_stage_model.decoder.mid.attn_1.norm.weight', 'first_stage_model.decoder.mid.attn_1.norm.bias', 'first_stage_model.decoder.mid.attn_1.q.weight', 'first_stage_model.decoder.mid.attn_1.q.bias', 'first_stage_model.decoder.mid.attn_1.k.weight', 'first_stage_model.decoder.mid.attn_1.k.bias', 'first_stage_model.decoder.mid.attn_1.v.weight', 'first_stage_model.decoder.mid.attn_1.v.bias', 'first_stage_model.decoder.mid.attn_1.proj_out.weight', 'first_stage_model.decoder.mid.attn_1.proj_out.bias', 'first_stage_model.decoder.mid.block_2.norm1.weight', 'first_stage_model.decoder.mid.block_2.norm1.bias', 'first_stage_model.decoder.mid.block_2.conv1.weight', 'first_stage_model.decoder.mid.block_2.conv1.bias', 'first_stage_model.decoder.mid.block_2.norm2.weight', 'first_stage_model.decoder.mid.block_2.norm2.bias', 'first_stage_model.decoder.mid.block_2.conv2.weight', 'first_stage_model.decoder.mid.block_2.conv2.bias', 'first_stage_model.decoder.up.0.block.0.norm1.weight', 'first_stage_model.decoder.up.0.block.0.norm1.bias', 'first_stage_model.decoder.up.0.block.0.conv1.weight', 'first_stage_model.decoder.up.0.block.0.conv1.bias', 'first_stage_model.decoder.up.0.block.0.norm2.weight', 'first_stage_model.decoder.up.0.block.0.norm2.bias', 'first_stage_model.decoder.up.0.block.0.conv2.weight', 'first_stage_model.decoder.up.0.block.0.conv2.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.weight', 'first_stage_model.decoder.up.0.block.1.norm1.bias', 'first_stage_model.decoder.up.0.block.1.conv1.weight', 'first_stage_model.decoder.up.0.block.1.conv1.bias', 'first_stage_model.decoder.up.0.block.1.norm2.weight', 'first_stage_model.decoder.up.0.block.1.norm2.bias', 'first_stage_model.decoder.up.0.block.1.conv2.weight', 'first_stage_model.decoder.up.0.block.1.conv2.bias', 'first_stage_model.decoder.up.0.block.2.norm1.weight', 'first_stage_model.decoder.up.0.block.2.norm1.bias', 'first_stage_model.decoder.up.0.block.2.conv1.weight', 'first_stage_model.decoder.up.0.block.2.conv1.bias', 'first_stage_model.decoder.up.0.block.2.norm2.weight', 'first_stage_model.decoder.up.0.block.2.norm2.bias', 'first_stage_model.decoder.up.0.block.2.conv2.weight', 'first_stage_model.decoder.up.0.block.2.conv2.bias', 'first_stage_model.decoder.up.1.block.0.norm1.weight', 'first_stage_model.decoder.up.1.block.0.norm1.bias', 'first_stage_model.decoder.up.1.block.0.conv1.weight', 'first_stage_model.decoder.up.1.block.0.conv1.bias', 'first_stage_model.decoder.up.1.block.0.norm2.weight', 'first_stage_model.decoder.up.1.block.0.norm2.bias', 'first_stage_model.decoder.up.1.block.0.conv2.weight', 'first_stage_model.decoder.up.1.block.0.conv2.bias', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.1.block.1.norm1.weight', 'first_stage_model.decoder.up.1.block.1.norm1.bias', 'first_stage_model.decoder.up.1.block.1.conv1.weight', 'first_stage_model.decoder.up.1.block.1.conv1.bias', 'first_stage_model.decoder.up.1.block.1.norm2.weight', 'first_stage_model.decoder.up.1.block.1.norm2.bias', 'first_stage_model.decoder.up.1.block.1.conv2.weight', 'first_stage_model.decoder.up.1.block.1.conv2.bias', 'first_stage_model.decoder.up.1.block.2.norm1.weight', 'first_stage_model.decoder.up.1.block.2.norm1.bias', 'first_stage_model.decoder.up.1.block.2.conv1.weight', 'first_stage_model.decoder.up.1.block.2.conv1.bias', 'first_stage_model.decoder.up.1.block.2.norm2.weight', 'first_stage_model.decoder.up.1.block.2.norm2.bias', 'first_stage_model.decoder.up.1.block.2.conv2.weight', 'first_stage_model.decoder.up.1.block.2.conv2.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.weight', 'first_stage_model.decoder.up.2.block.0.norm1.bias', 'first_stage_model.decoder.up.2.block.0.conv1.weight', 'first_stage_model.decoder.up.2.block.0.conv1.bias', 'first_stage_model.decoder.up.2.block.0.norm2.weight', 'first_stage_model.decoder.up.2.block.0.norm2.bias', 'first_stage_model.decoder.up.2.block.0.conv2.weight', 'first_stage_model.decoder.up.2.block.0.conv2.bias', 'first_stage_model.decoder.up.2.block.1.norm1.weight', 'first_stage_model.decoder.up.2.block.1.norm1.bias', 'first_stage_model.decoder.up.2.block.1.conv1.weight', 'first_stage_model.decoder.up.2.block.1.conv1.bias', 'first_stage_model.decoder.up.2.block.1.norm2.weight', 'first_stage_model.decoder.up.2.block.1.norm2.bias', 'first_stage_model.decoder.up.2.block.1.conv2.weight', 'first_stage_model.decoder.up.2.block.1.conv2.bias', 'first_stage_model.decoder.up.2.block.2.norm1.weight', 'first_stage_model.decoder.up.2.block.2.norm1.bias', 'first_stage_model.decoder.up.2.block.2.conv1.weight', 'first_stage_model.decoder.up.2.block.2.conv1.bias', 'first_stage_model.decoder.up.2.block.2.norm2.weight', 'first_stage_model.decoder.up.2.block.2.norm2.bias', 'first_stage_model.decoder.up.2.block.2.conv2.weight', 'first_stage_model.decoder.up.2.block.2.conv2.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.weight', 'first_stage_model.decoder.up.3.block.0.norm1.bias', 'first_stage_model.decoder.up.3.block.0.conv1.weight', 'first_stage_model.decoder.up.3.block.0.conv1.bias', 'first_stage_model.decoder.up.3.block.0.norm2.weight', 'first_stage_model.decoder.up.3.block.0.norm2.bias', 'first_stage_model.decoder.up.3.block.0.conv2.weight', 'first_stage_model.decoder.up.3.block.0.conv2.bias', 'first_stage_model.decoder.up.3.block.1.norm1.weight', 'first_stage_model.decoder.up.3.block.1.norm1.bias', 'first_stage_model.decoder.up.3.block.1.conv1.weight', 'first_stage_model.decoder.up.3.block.1.conv1.bias', 'first_stage_model.decoder.up.3.block.1.norm2.weight', 'first_stage_model.decoder.up.3.block.1.norm2.bias', 'first_stage_model.decoder.up.3.block.1.conv2.weight', 'first_stage_model.decoder.up.3.block.1.conv2.bias', 'first_stage_model.decoder.up.3.block.2.norm1.weight', 'first_stage_model.decoder.up.3.block.2.norm1.bias', 'first_stage_model.decoder.up.3.block.2.conv1.weight', 'first_stage_model.decoder.up.3.block.2.conv1.bias', 'first_stage_model.decoder.up.3.block.2.norm2.weight', 'first_stage_model.decoder.up.3.block.2.norm2.bias', 'first_stage_model.decoder.up.3.block.2.conv2.weight', 'first_stage_model.decoder.up.3.block.2.conv2.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.weight', 'first_stage_model.decoder.norm_out.bias', 'first_stage_model.decoder.conv_out.weight', 'first_stage_model.decoder.conv_out.bias', 'first_stage_model.quant_conv.weight', 'first_stage_model.quant_conv.bias', 'first_stage_model.post_quant_conv.weight', 'first_stage_model.post_quant_conv.bias', 'cond_stage_model.transformer.text_model.embeddings.position_ids', 'cond_stage_model.transformer.text_model.embeddings.token_embedding.weight', 'cond_stage_model.transformer.text_model.embeddings.position_embedding.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias', 'cond_stage_model.transformer.text_model.final_layer_norm.weight', 'cond_stage_model.transformer.text_model.final_layer_norm.bias', 'control_model.time_embed.0.weight', 'control_model.time_embed.0.bias', 'control_model.time_embed.2.weight', 'control_model.time_embed.2.bias', 'control_model.input_blocks.0.0.weight', 'control_model.input_blocks.0.0.bias', 'control_model.input_blocks.1.0.in_layers.0.weight', 'control_model.input_blocks.1.0.in_layers.0.bias', 'control_model.input_blocks.1.0.in_layers.2.weight', 'control_model.input_blocks.1.0.in_layers.2.bias', 'control_model.input_blocks.1.0.emb_layers.1.weight', 'control_model.input_blocks.1.0.emb_layers.1.bias', 'control_model.input_blocks.1.0.out_layers.0.weight', 'control_model.input_blocks.1.0.out_layers.0.bias', 'control_model.input_blocks.1.0.out_layers.3.weight', 'control_model.input_blocks.1.0.out_layers.3.bias', 'control_model.input_blocks.1.1.norm.weight', 'control_model.input_blocks.1.1.norm.bias', 'control_model.input_blocks.1.1.proj_in.weight', 'control_model.input_blocks.1.1.proj_in.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.1.1.proj_out.weight', 'control_model.input_blocks.1.1.proj_out.bias', 'control_model.input_blocks.2.0.in_layers.0.weight', 'control_model.input_blocks.2.0.in_layers.0.bias', 'control_model.input_blocks.2.0.in_layers.2.weight', 'control_model.input_blocks.2.0.in_layers.2.bias', 'control_model.input_blocks.2.0.emb_layers.1.weight', 'control_model.input_blocks.2.0.emb_layers.1.bias', 'control_model.input_blocks.2.0.out_layers.0.weight', 'control_model.input_blocks.2.0.out_layers.0.bias', 'control_model.input_blocks.2.0.out_layers.3.weight', 'control_model.input_blocks.2.0.out_layers.3.bias', 'control_model.input_blocks.2.1.norm.weight', 'control_model.input_blocks.2.1.norm.bias', 'control_model.input_blocks.2.1.proj_in.weight', 'control_model.input_blocks.2.1.proj_in.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.2.1.proj_out.weight', 'control_model.input_blocks.2.1.proj_out.bias', 'control_model.input_blocks.3.0.op.weight', 'control_model.input_blocks.3.0.op.bias', 'control_model.input_blocks.4.0.in_layers.0.weight', 'control_model.input_blocks.4.0.in_layers.0.bias', 'control_model.input_blocks.4.0.in_layers.2.weight', 'control_model.input_blocks.4.0.in_layers.2.bias', 'control_model.input_blocks.4.0.emb_layers.1.weight', 'control_model.input_blocks.4.0.emb_layers.1.bias', 'control_model.input_blocks.4.0.out_layers.0.weight', 'control_model.input_blocks.4.0.out_layers.0.bias', 'control_model.input_blocks.4.0.out_layers.3.weight', 'control_model.input_blocks.4.0.out_layers.3.bias', 'control_model.input_blocks.4.0.skip_connection.weight', 'control_model.input_blocks.4.0.skip_connection.bias', 'control_model.input_blocks.4.1.norm.weight', 'control_model.input_blocks.4.1.norm.bias', 'control_model.input_blocks.4.1.proj_in.weight', 'control_model.input_blocks.4.1.proj_in.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.4.1.proj_out.weight', 'control_model.input_blocks.4.1.proj_out.bias', 'control_model.input_blocks.5.0.in_layers.0.weight', 'control_model.input_blocks.5.0.in_layers.0.bias', 'control_model.input_blocks.5.0.in_layers.2.weight', 'control_model.input_blocks.5.0.in_layers.2.bias', 'control_model.input_blocks.5.0.emb_layers.1.weight', 'control_model.input_blocks.5.0.emb_layers.1.bias', 'control_model.input_blocks.5.0.out_layers.0.weight', 'control_model.input_blocks.5.0.out_layers.0.bias', 'control_model.input_blocks.5.0.out_layers.3.weight', 'control_model.input_blocks.5.0.out_layers.3.bias', 'control_model.input_blocks.5.1.norm.weight', 'control_model.input_blocks.5.1.norm.bias', 'control_model.input_blocks.5.1.proj_in.weight', 'control_model.input_blocks.5.1.proj_in.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.5.1.proj_out.weight', 'control_model.input_blocks.5.1.proj_out.bias', 'control_model.input_blocks.6.0.op.weight', 'control_model.input_blocks.6.0.op.bias', 'control_model.input_blocks.7.0.in_layers.0.weight', 'control_model.input_blocks.7.0.in_layers.0.bias', 'control_model.input_blocks.7.0.in_layers.2.weight', 'control_model.input_blocks.7.0.in_layers.2.bias', 'control_model.input_blocks.7.0.emb_layers.1.weight', 'control_model.input_blocks.7.0.emb_layers.1.bias', 'control_model.input_blocks.7.0.out_layers.0.weight', 'control_model.input_blocks.7.0.out_layers.0.bias', 'control_model.input_blocks.7.0.out_layers.3.weight', 'control_model.input_blocks.7.0.out_layers.3.bias', 'control_model.input_blocks.7.0.skip_connection.weight', 'control_model.input_blocks.7.0.skip_connection.bias', 'control_model.input_blocks.7.1.norm.weight', 'control_model.input_blocks.7.1.norm.bias', 'control_model.input_blocks.7.1.proj_in.weight', 'control_model.input_blocks.7.1.proj_in.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.7.1.proj_out.weight', 'control_model.input_blocks.7.1.proj_out.bias', 'control_model.input_blocks.8.0.in_layers.0.weight', 'control_model.input_blocks.8.0.in_layers.0.bias', 'control_model.input_blocks.8.0.in_layers.2.weight', 'control_model.input_blocks.8.0.in_layers.2.bias', 'control_model.input_blocks.8.0.emb_layers.1.weight', 'control_model.input_blocks.8.0.emb_layers.1.bias', 'control_model.input_blocks.8.0.out_layers.0.weight', 'control_model.input_blocks.8.0.out_layers.0.bias', 'control_model.input_blocks.8.0.out_layers.3.weight', 'control_model.input_blocks.8.0.out_layers.3.bias', 'control_model.input_blocks.8.1.norm.weight', 'control_model.input_blocks.8.1.norm.bias', 'control_model.input_blocks.8.1.proj_in.weight', 'control_model.input_blocks.8.1.proj_in.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.8.1.proj_out.weight', 'control_model.input_blocks.8.1.proj_out.bias', 'control_model.input_blocks.9.0.op.weight', 'control_model.input_blocks.9.0.op.bias', 'control_model.input_blocks.10.0.in_layers.0.weight', 'control_model.input_blocks.10.0.in_layers.0.bias', 'control_model.input_blocks.10.0.in_layers.2.weight', 'control_model.input_blocks.10.0.in_layers.2.bias', 'control_model.input_blocks.10.0.emb_layers.1.weight', 'control_model.input_blocks.10.0.emb_layers.1.bias', 'control_model.input_blocks.10.0.out_layers.0.weight', 'control_model.input_blocks.10.0.out_layers.0.bias', 'control_model.input_blocks.10.0.out_layers.3.weight', 'control_model.input_blocks.10.0.out_layers.3.bias', 'control_model.input_blocks.11.0.in_layers.0.weight', 'control_model.input_blocks.11.0.in_layers.0.bias', 'control_model.input_blocks.11.0.in_layers.2.weight', 'control_model.input_blocks.11.0.in_layers.2.bias', 'control_model.input_blocks.11.0.emb_layers.1.weight', 'control_model.input_blocks.11.0.emb_layers.1.bias', 'control_model.input_blocks.11.0.out_layers.0.weight', 'control_model.input_blocks.11.0.out_layers.0.bias', 'control_model.input_blocks.11.0.out_layers.3.weight', 'control_model.input_blocks.11.0.out_layers.3.bias', 'control_model.zero_convs.0.0.weight', 'control_model.zero_convs.0.0.bias', 'control_model.zero_convs.1.0.weight', 'control_model.zero_convs.1.0.bias', 'control_model.zero_convs.2.0.weight', 'control_model.zero_convs.2.0.bias', 'control_model.zero_convs.3.0.weight', 'control_model.zero_convs.3.0.bias', 'control_model.zero_convs.4.0.weight', 'control_model.zero_convs.4.0.bias', 'control_model.zero_convs.5.0.weight', 'control_model.zero_convs.5.0.bias', 'control_model.zero_convs.6.0.weight', 'control_model.zero_convs.6.0.bias', 'control_model.zero_convs.7.0.weight', 'control_model.zero_convs.7.0.bias', 'control_model.zero_convs.8.0.weight', 'control_model.zero_convs.8.0.bias', 'control_model.zero_convs.9.0.weight', 'control_model.zero_convs.9.0.bias', 'control_model.zero_convs.10.0.weight', 'control_model.zero_convs.10.0.bias', 'control_model.zero_convs.11.0.weight', 'control_model.zero_convs.11.0.bias', 'control_model.input_hint_block.0.weight', 'control_model.input_hint_block.0.bias', 'control_model.input_hint_block.2.weight', 'control_model.input_hint_block.2.bias', 'control_model.input_hint_block.4.weight', 'control_model.input_hint_block.4.bias', 'control_model.input_hint_block.6.weight', 'control_model.input_hint_block.6.bias', 'control_model.input_hint_block.8.weight', 'control_model.input_hint_block.8.bias', 'control_model.input_hint_block.10.weight', 'control_model.input_hint_block.10.bias', 'control_model.input_hint_block.12.weight', 'control_model.input_hint_block.12.bias', 'control_model.input_hint_block.14.weight', 'control_model.input_hint_block.14.bias', 'control_model.middle_block.0.in_layers.0.weight', 'control_model.middle_block.0.in_layers.0.bias', 'control_model.middle_block.0.in_layers.2.weight', 'control_model.middle_block.0.in_layers.2.bias', 'control_model.middle_block.0.emb_layers.1.weight', 'control_model.middle_block.0.emb_layers.1.bias', 'control_model.middle_block.0.out_layers.0.weight', 'control_model.middle_block.0.out_layers.0.bias', 'control_model.middle_block.0.out_layers.3.weight', 'control_model.middle_block.0.out_layers.3.bias', 'control_model.middle_block.1.norm.weight', 'control_model.middle_block.1.norm.bias', 'control_model.middle_block.1.proj_in.weight', 'control_model.middle_block.1.proj_in.bias', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model.middle_block.1.proj_out.weight', 'control_model.middle_block.1.proj_out.bias', 'control_model.middle_block.2.in_layers.0.weight', 'control_model.middle_block.2.in_layers.0.bias', 'control_model.middle_block.2.in_layers.2.weight', 'control_model.middle_block.2.in_layers.2.bias', 'control_model.middle_block.2.emb_layers.1.weight', 'control_model.middle_block.2.emb_layers.1.bias', 'control_model.middle_block.2.out_layers.0.weight', 'control_model.middle_block.2.out_layers.0.bias', 'control_model.middle_block.2.out_layers.3.weight', 'control_model.middle_block.2.out_layers.3.bias', 'control_model.middle_block_out.0.weight', 'control_model.middle_block_out.0.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(control_net_ckpt.keys())\n",
    "convert_state_dict_to_jsonl(control_net_ckpt, \"control_sd15_canny_weights.jsonl\")\n",
    "# json으로 따서 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628969f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT\n",
    "new_canny_checkpoint = {} \n",
    "for k, v in control_net_ckpt.items():\n",
    "    if \"control_model\" in k:\n",
    "        _k = k.replace(\"control_model.\", \"\")\n",
    "        if \"zero_convs\" in _k:\n",
    "            v = torch.zeros_like(v)\n",
    "        \n",
    "        new_canny_checkpoint[_k] = v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc5ed2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['time_embed.0.weight', 'time_embed.0.bias', 'time_embed.2.weight', 'time_embed.2.bias', 'input_blocks.0.0.weight', 'input_blocks.0.0.bias', 'input_blocks.1.0.in_layers.0.weight', 'input_blocks.1.0.in_layers.0.bias', 'input_blocks.1.0.in_layers.2.weight', 'input_blocks.1.0.in_layers.2.bias', 'input_blocks.1.0.emb_layers.1.weight', 'input_blocks.1.0.emb_layers.1.bias', 'input_blocks.1.0.out_layers.0.weight', 'input_blocks.1.0.out_layers.0.bias', 'input_blocks.1.0.out_layers.3.weight', 'input_blocks.1.0.out_layers.3.bias', 'input_blocks.1.1.norm.weight', 'input_blocks.1.1.norm.bias', 'input_blocks.1.1.proj_in.weight', 'input_blocks.1.1.proj_in.bias', 'input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.1.1.transformer_blocks.0.norm1.weight', 'input_blocks.1.1.transformer_blocks.0.norm1.bias', 'input_blocks.1.1.transformer_blocks.0.norm2.weight', 'input_blocks.1.1.transformer_blocks.0.norm2.bias', 'input_blocks.1.1.transformer_blocks.0.norm3.weight', 'input_blocks.1.1.transformer_blocks.0.norm3.bias', 'input_blocks.1.1.proj_out.weight', 'input_blocks.1.1.proj_out.bias', 'input_blocks.2.0.in_layers.0.weight', 'input_blocks.2.0.in_layers.0.bias', 'input_blocks.2.0.in_layers.2.weight', 'input_blocks.2.0.in_layers.2.bias', 'input_blocks.2.0.emb_layers.1.weight', 'input_blocks.2.0.emb_layers.1.bias', 'input_blocks.2.0.out_layers.0.weight', 'input_blocks.2.0.out_layers.0.bias', 'input_blocks.2.0.out_layers.3.weight', 'input_blocks.2.0.out_layers.3.bias', 'input_blocks.2.1.norm.weight', 'input_blocks.2.1.norm.bias', 'input_blocks.2.1.proj_in.weight', 'input_blocks.2.1.proj_in.bias', 'input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.2.1.transformer_blocks.0.norm1.weight', 'input_blocks.2.1.transformer_blocks.0.norm1.bias', 'input_blocks.2.1.transformer_blocks.0.norm2.weight', 'input_blocks.2.1.transformer_blocks.0.norm2.bias', 'input_blocks.2.1.transformer_blocks.0.norm3.weight', 'input_blocks.2.1.transformer_blocks.0.norm3.bias', 'input_blocks.2.1.proj_out.weight', 'input_blocks.2.1.proj_out.bias', 'input_blocks.3.0.op.weight', 'input_blocks.3.0.op.bias', 'input_blocks.4.0.in_layers.0.weight', 'input_blocks.4.0.in_layers.0.bias', 'input_blocks.4.0.in_layers.2.weight', 'input_blocks.4.0.in_layers.2.bias', 'input_blocks.4.0.emb_layers.1.weight', 'input_blocks.4.0.emb_layers.1.bias', 'input_blocks.4.0.out_layers.0.weight', 'input_blocks.4.0.out_layers.0.bias', 'input_blocks.4.0.out_layers.3.weight', 'input_blocks.4.0.out_layers.3.bias', 'input_blocks.4.0.skip_connection.weight', 'input_blocks.4.0.skip_connection.bias', 'input_blocks.4.1.norm.weight', 'input_blocks.4.1.norm.bias', 'input_blocks.4.1.proj_in.weight', 'input_blocks.4.1.proj_in.bias', 'input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.4.1.transformer_blocks.0.norm1.weight', 'input_blocks.4.1.transformer_blocks.0.norm1.bias', 'input_blocks.4.1.transformer_blocks.0.norm2.weight', 'input_blocks.4.1.transformer_blocks.0.norm2.bias', 'input_blocks.4.1.transformer_blocks.0.norm3.weight', 'input_blocks.4.1.transformer_blocks.0.norm3.bias', 'input_blocks.4.1.proj_out.weight', 'input_blocks.4.1.proj_out.bias', 'input_blocks.5.0.in_layers.0.weight', 'input_blocks.5.0.in_layers.0.bias', 'input_blocks.5.0.in_layers.2.weight', 'input_blocks.5.0.in_layers.2.bias', 'input_blocks.5.0.emb_layers.1.weight', 'input_blocks.5.0.emb_layers.1.bias', 'input_blocks.5.0.out_layers.0.weight', 'input_blocks.5.0.out_layers.0.bias', 'input_blocks.5.0.out_layers.3.weight', 'input_blocks.5.0.out_layers.3.bias', 'input_blocks.5.1.norm.weight', 'input_blocks.5.1.norm.bias', 'input_blocks.5.1.proj_in.weight', 'input_blocks.5.1.proj_in.bias', 'input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.5.1.transformer_blocks.0.norm1.weight', 'input_blocks.5.1.transformer_blocks.0.norm1.bias', 'input_blocks.5.1.transformer_blocks.0.norm2.weight', 'input_blocks.5.1.transformer_blocks.0.norm2.bias', 'input_blocks.5.1.transformer_blocks.0.norm3.weight', 'input_blocks.5.1.transformer_blocks.0.norm3.bias', 'input_blocks.5.1.proj_out.weight', 'input_blocks.5.1.proj_out.bias', 'input_blocks.6.0.op.weight', 'input_blocks.6.0.op.bias', 'input_blocks.7.0.in_layers.0.weight', 'input_blocks.7.0.in_layers.0.bias', 'input_blocks.7.0.in_layers.2.weight', 'input_blocks.7.0.in_layers.2.bias', 'input_blocks.7.0.emb_layers.1.weight', 'input_blocks.7.0.emb_layers.1.bias', 'input_blocks.7.0.out_layers.0.weight', 'input_blocks.7.0.out_layers.0.bias', 'input_blocks.7.0.out_layers.3.weight', 'input_blocks.7.0.out_layers.3.bias', 'input_blocks.7.0.skip_connection.weight', 'input_blocks.7.0.skip_connection.bias', 'input_blocks.7.1.norm.weight', 'input_blocks.7.1.norm.bias', 'input_blocks.7.1.proj_in.weight', 'input_blocks.7.1.proj_in.bias', 'input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.7.1.transformer_blocks.0.norm1.weight', 'input_blocks.7.1.transformer_blocks.0.norm1.bias', 'input_blocks.7.1.transformer_blocks.0.norm2.weight', 'input_blocks.7.1.transformer_blocks.0.norm2.bias', 'input_blocks.7.1.transformer_blocks.0.norm3.weight', 'input_blocks.7.1.transformer_blocks.0.norm3.bias', 'input_blocks.7.1.proj_out.weight', 'input_blocks.7.1.proj_out.bias', 'input_blocks.8.0.in_layers.0.weight', 'input_blocks.8.0.in_layers.0.bias', 'input_blocks.8.0.in_layers.2.weight', 'input_blocks.8.0.in_layers.2.bias', 'input_blocks.8.0.emb_layers.1.weight', 'input_blocks.8.0.emb_layers.1.bias', 'input_blocks.8.0.out_layers.0.weight', 'input_blocks.8.0.out_layers.0.bias', 'input_blocks.8.0.out_layers.3.weight', 'input_blocks.8.0.out_layers.3.bias', 'input_blocks.8.1.norm.weight', 'input_blocks.8.1.norm.bias', 'input_blocks.8.1.proj_in.weight', 'input_blocks.8.1.proj_in.bias', 'input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.8.1.transformer_blocks.0.norm1.weight', 'input_blocks.8.1.transformer_blocks.0.norm1.bias', 'input_blocks.8.1.transformer_blocks.0.norm2.weight', 'input_blocks.8.1.transformer_blocks.0.norm2.bias', 'input_blocks.8.1.transformer_blocks.0.norm3.weight', 'input_blocks.8.1.transformer_blocks.0.norm3.bias', 'input_blocks.8.1.proj_out.weight', 'input_blocks.8.1.proj_out.bias', 'input_blocks.9.0.op.weight', 'input_blocks.9.0.op.bias', 'input_blocks.10.0.in_layers.0.weight', 'input_blocks.10.0.in_layers.0.bias', 'input_blocks.10.0.in_layers.2.weight', 'input_blocks.10.0.in_layers.2.bias', 'input_blocks.10.0.emb_layers.1.weight', 'input_blocks.10.0.emb_layers.1.bias', 'input_blocks.10.0.out_layers.0.weight', 'input_blocks.10.0.out_layers.0.bias', 'input_blocks.10.0.out_layers.3.weight', 'input_blocks.10.0.out_layers.3.bias', 'input_blocks.11.0.in_layers.0.weight', 'input_blocks.11.0.in_layers.0.bias', 'input_blocks.11.0.in_layers.2.weight', 'input_blocks.11.0.in_layers.2.bias', 'input_blocks.11.0.emb_layers.1.weight', 'input_blocks.11.0.emb_layers.1.bias', 'input_blocks.11.0.out_layers.0.weight', 'input_blocks.11.0.out_layers.0.bias', 'input_blocks.11.0.out_layers.3.weight', 'input_blocks.11.0.out_layers.3.bias', 'zero_convs.0.0.weight', 'zero_convs.0.0.bias', 'zero_convs.1.0.weight', 'zero_convs.1.0.bias', 'zero_convs.2.0.weight', 'zero_convs.2.0.bias', 'zero_convs.3.0.weight', 'zero_convs.3.0.bias', 'zero_convs.4.0.weight', 'zero_convs.4.0.bias', 'zero_convs.5.0.weight', 'zero_convs.5.0.bias', 'zero_convs.6.0.weight', 'zero_convs.6.0.bias', 'zero_convs.7.0.weight', 'zero_convs.7.0.bias', 'zero_convs.8.0.weight', 'zero_convs.8.0.bias', 'zero_convs.9.0.weight', 'zero_convs.9.0.bias', 'zero_convs.10.0.weight', 'zero_convs.10.0.bias', 'zero_convs.11.0.weight', 'zero_convs.11.0.bias', 'input_hint_block.0.weight', 'input_hint_block.0.bias', 'input_hint_block.2.weight', 'input_hint_block.2.bias', 'input_hint_block.4.weight', 'input_hint_block.4.bias', 'input_hint_block.6.weight', 'input_hint_block.6.bias', 'input_hint_block.8.weight', 'input_hint_block.8.bias', 'input_hint_block.10.weight', 'input_hint_block.10.bias', 'input_hint_block.12.weight', 'input_hint_block.12.bias', 'input_hint_block.14.weight', 'input_hint_block.14.bias', 'middle_block.0.in_layers.0.weight', 'middle_block.0.in_layers.0.bias', 'middle_block.0.in_layers.2.weight', 'middle_block.0.in_layers.2.bias', 'middle_block.0.emb_layers.1.weight', 'middle_block.0.emb_layers.1.bias', 'middle_block.0.out_layers.0.weight', 'middle_block.0.out_layers.0.bias', 'middle_block.0.out_layers.3.weight', 'middle_block.0.out_layers.3.bias', 'middle_block.1.norm.weight', 'middle_block.1.norm.bias', 'middle_block.1.proj_in.weight', 'middle_block.1.proj_in.bias', 'middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'middle_block.1.transformer_blocks.0.ff.net.2.weight', 'middle_block.1.transformer_blocks.0.ff.net.2.bias', 'middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'middle_block.1.transformer_blocks.0.norm1.weight', 'middle_block.1.transformer_blocks.0.norm1.bias', 'middle_block.1.transformer_blocks.0.norm2.weight', 'middle_block.1.transformer_blocks.0.norm2.bias', 'middle_block.1.transformer_blocks.0.norm3.weight', 'middle_block.1.transformer_blocks.0.norm3.bias', 'middle_block.1.proj_out.weight', 'middle_block.1.proj_out.bias', 'middle_block.2.in_layers.0.weight', 'middle_block.2.in_layers.0.bias', 'middle_block.2.in_layers.2.weight', 'middle_block.2.in_layers.2.bias', 'middle_block.2.emb_layers.1.weight', 'middle_block.2.emb_layers.1.bias', 'middle_block.2.out_layers.0.weight', 'middle_block.2.out_layers.0.bias', 'middle_block.2.out_layers.3.weight', 'middle_block.2.out_layers.3.bias', 'middle_block_out.0.weight', 'middle_block_out.0.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(new_canny_checkpoint.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f89df5",
   "metadata": {},
   "source": [
    "# 이종 ControlNet Signaling try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c1afa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_scratch_path = os.path.join(\n",
    "    \"/workspace/code/3DAnything/zero123/configs/canny-edge_variant_i2t_multi.yaml\"\n",
    ")\n",
    "control_scratch_cfg= OmegaConf.load(control_scratch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a71cdfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "omegaconf.dictconfig.DictConfig"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(control_scratch_cfg.pre_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d784d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiControlNet: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "model_scratch = instantiate_from_config(control_scratch_cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6beb8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ControlNet(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (zero_convs): ModuleList(\n",
       "    (0-3): 4 x TimestepEmbedSequential(\n",
       "      (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (4-6): 3 x TimestepEmbedSequential(\n",
       "      (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (7-11): 5 x TimestepEmbedSequential(\n",
       "      (0): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (input_hint_block): TimestepEmbedSequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): SiLU()\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): SiLU()\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): SiLU()\n",
       "    (8): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): SiLU()\n",
       "    (10): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): SiLU()\n",
       "    (12): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (13): SiLU()\n",
       "    (14): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (middle_block_out): TimestepEmbedSequential(\n",
       "    (0): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scratch.control_model_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfc25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time_embed.0.weight', 'time_embed.0.bias', 'time_embed.2.weight', 'time_embed.2.bias', 'input_blocks.0.0.weight', 'input_blocks.0.0.bias', 'input_blocks.1.0.in_layers.0.weight', 'input_blocks.1.0.in_layers.0.bias', 'input_blocks.1.0.in_layers.2.weight', 'input_blocks.1.0.in_layers.2.bias', 'input_blocks.1.0.emb_layers.1.weight', 'input_blocks.1.0.emb_layers.1.bias', 'input_blocks.1.0.out_layers.0.weight', 'input_blocks.1.0.out_layers.0.bias', 'input_blocks.1.0.out_layers.3.weight', 'input_blocks.1.0.out_layers.3.bias', 'input_blocks.1.1.norm.weight', 'input_blocks.1.1.norm.bias', 'input_blocks.1.1.proj_in.weight', 'input_blocks.1.1.proj_in.bias', 'input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.1.1.transformer_blocks.0.norm1.weight', 'input_blocks.1.1.transformer_blocks.0.norm1.bias', 'input_blocks.1.1.transformer_blocks.0.norm2.weight', 'input_blocks.1.1.transformer_blocks.0.norm2.bias', 'input_blocks.1.1.transformer_blocks.0.norm3.weight', 'input_blocks.1.1.transformer_blocks.0.norm3.bias', 'input_blocks.1.1.proj_out.weight', 'input_blocks.1.1.proj_out.bias', 'input_blocks.2.0.in_layers.0.weight', 'input_blocks.2.0.in_layers.0.bias', 'input_blocks.2.0.in_layers.2.weight', 'input_blocks.2.0.in_layers.2.bias', 'input_blocks.2.0.emb_layers.1.weight', 'input_blocks.2.0.emb_layers.1.bias', 'input_blocks.2.0.out_layers.0.weight', 'input_blocks.2.0.out_layers.0.bias', 'input_blocks.2.0.out_layers.3.weight', 'input_blocks.2.0.out_layers.3.bias', 'input_blocks.2.1.norm.weight', 'input_blocks.2.1.norm.bias', 'input_blocks.2.1.proj_in.weight', 'input_blocks.2.1.proj_in.bias', 'input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.2.1.transformer_blocks.0.norm1.weight', 'input_blocks.2.1.transformer_blocks.0.norm1.bias', 'input_blocks.2.1.transformer_blocks.0.norm2.weight', 'input_blocks.2.1.transformer_blocks.0.norm2.bias', 'input_blocks.2.1.transformer_blocks.0.norm3.weight', 'input_blocks.2.1.transformer_blocks.0.norm3.bias', 'input_blocks.2.1.proj_out.weight', 'input_blocks.2.1.proj_out.bias', 'input_blocks.3.0.op.weight', 'input_blocks.3.0.op.bias', 'input_blocks.4.0.in_layers.0.weight', 'input_blocks.4.0.in_layers.0.bias', 'input_blocks.4.0.in_layers.2.weight', 'input_blocks.4.0.in_layers.2.bias', 'input_blocks.4.0.emb_layers.1.weight', 'input_blocks.4.0.emb_layers.1.bias', 'input_blocks.4.0.out_layers.0.weight', 'input_blocks.4.0.out_layers.0.bias', 'input_blocks.4.0.out_layers.3.weight', 'input_blocks.4.0.out_layers.3.bias', 'input_blocks.4.0.skip_connection.weight', 'input_blocks.4.0.skip_connection.bias', 'input_blocks.4.1.norm.weight', 'input_blocks.4.1.norm.bias', 'input_blocks.4.1.proj_in.weight', 'input_blocks.4.1.proj_in.bias', 'input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.4.1.transformer_blocks.0.norm1.weight', 'input_blocks.4.1.transformer_blocks.0.norm1.bias', 'input_blocks.4.1.transformer_blocks.0.norm2.weight', 'input_blocks.4.1.transformer_blocks.0.norm2.bias', 'input_blocks.4.1.transformer_blocks.0.norm3.weight', 'input_blocks.4.1.transformer_blocks.0.norm3.bias', 'input_blocks.4.1.proj_out.weight', 'input_blocks.4.1.proj_out.bias', 'input_blocks.5.0.in_layers.0.weight', 'input_blocks.5.0.in_layers.0.bias', 'input_blocks.5.0.in_layers.2.weight', 'input_blocks.5.0.in_layers.2.bias', 'input_blocks.5.0.emb_layers.1.weight', 'input_blocks.5.0.emb_layers.1.bias', 'input_blocks.5.0.out_layers.0.weight', 'input_blocks.5.0.out_layers.0.bias', 'input_blocks.5.0.out_layers.3.weight', 'input_blocks.5.0.out_layers.3.bias', 'input_blocks.5.1.norm.weight', 'input_blocks.5.1.norm.bias', 'input_blocks.5.1.proj_in.weight', 'input_blocks.5.1.proj_in.bias', 'input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.5.1.transformer_blocks.0.norm1.weight', 'input_blocks.5.1.transformer_blocks.0.norm1.bias', 'input_blocks.5.1.transformer_blocks.0.norm2.weight', 'input_blocks.5.1.transformer_blocks.0.norm2.bias', 'input_blocks.5.1.transformer_blocks.0.norm3.weight', 'input_blocks.5.1.transformer_blocks.0.norm3.bias', 'input_blocks.5.1.proj_out.weight', 'input_blocks.5.1.proj_out.bias', 'input_blocks.6.0.op.weight', 'input_blocks.6.0.op.bias', 'input_blocks.7.0.in_layers.0.weight', 'input_blocks.7.0.in_layers.0.bias', 'input_blocks.7.0.in_layers.2.weight', 'input_blocks.7.0.in_layers.2.bias', 'input_blocks.7.0.emb_layers.1.weight', 'input_blocks.7.0.emb_layers.1.bias', 'input_blocks.7.0.out_layers.0.weight', 'input_blocks.7.0.out_layers.0.bias', 'input_blocks.7.0.out_layers.3.weight', 'input_blocks.7.0.out_layers.3.bias', 'input_blocks.7.0.skip_connection.weight', 'input_blocks.7.0.skip_connection.bias', 'input_blocks.7.1.norm.weight', 'input_blocks.7.1.norm.bias', 'input_blocks.7.1.proj_in.weight', 'input_blocks.7.1.proj_in.bias', 'input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.7.1.transformer_blocks.0.norm1.weight', 'input_blocks.7.1.transformer_blocks.0.norm1.bias', 'input_blocks.7.1.transformer_blocks.0.norm2.weight', 'input_blocks.7.1.transformer_blocks.0.norm2.bias', 'input_blocks.7.1.transformer_blocks.0.norm3.weight', 'input_blocks.7.1.transformer_blocks.0.norm3.bias', 'input_blocks.7.1.proj_out.weight', 'input_blocks.7.1.proj_out.bias', 'input_blocks.8.0.in_layers.0.weight', 'input_blocks.8.0.in_layers.0.bias', 'input_blocks.8.0.in_layers.2.weight', 'input_blocks.8.0.in_layers.2.bias', 'input_blocks.8.0.emb_layers.1.weight', 'input_blocks.8.0.emb_layers.1.bias', 'input_blocks.8.0.out_layers.0.weight', 'input_blocks.8.0.out_layers.0.bias', 'input_blocks.8.0.out_layers.3.weight', 'input_blocks.8.0.out_layers.3.bias', 'input_blocks.8.1.norm.weight', 'input_blocks.8.1.norm.bias', 'input_blocks.8.1.proj_in.weight', 'input_blocks.8.1.proj_in.bias', 'input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'input_blocks.8.1.transformer_blocks.0.norm1.weight', 'input_blocks.8.1.transformer_blocks.0.norm1.bias', 'input_blocks.8.1.transformer_blocks.0.norm2.weight', 'input_blocks.8.1.transformer_blocks.0.norm2.bias', 'input_blocks.8.1.transformer_blocks.0.norm3.weight', 'input_blocks.8.1.transformer_blocks.0.norm3.bias', 'input_blocks.8.1.proj_out.weight', 'input_blocks.8.1.proj_out.bias', 'input_blocks.9.0.op.weight', 'input_blocks.9.0.op.bias', 'input_blocks.10.0.in_layers.0.weight', 'input_blocks.10.0.in_layers.0.bias', 'input_blocks.10.0.in_layers.2.weight', 'input_blocks.10.0.in_layers.2.bias', 'input_blocks.10.0.emb_layers.1.weight', 'input_blocks.10.0.emb_layers.1.bias', 'input_blocks.10.0.out_layers.0.weight', 'input_blocks.10.0.out_layers.0.bias', 'input_blocks.10.0.out_layers.3.weight', 'input_blocks.10.0.out_layers.3.bias', 'input_blocks.11.0.in_layers.0.weight', 'input_blocks.11.0.in_layers.0.bias', 'input_blocks.11.0.in_layers.2.weight', 'input_blocks.11.0.in_layers.2.bias', 'input_blocks.11.0.emb_layers.1.weight', 'input_blocks.11.0.emb_layers.1.bias', 'input_blocks.11.0.out_layers.0.weight', 'input_blocks.11.0.out_layers.0.bias', 'input_blocks.11.0.out_layers.3.weight', 'input_blocks.11.0.out_layers.3.bias', 'zero_convs.0.0.weight', 'zero_convs.0.0.bias', 'zero_convs.1.0.weight', 'zero_convs.1.0.bias', 'zero_convs.2.0.weight', 'zero_convs.2.0.bias', 'zero_convs.3.0.weight', 'zero_convs.3.0.bias', 'zero_convs.4.0.weight', 'zero_convs.4.0.bias', 'zero_convs.5.0.weight', 'zero_convs.5.0.bias', 'zero_convs.6.0.weight', 'zero_convs.6.0.bias', 'zero_convs.7.0.weight', 'zero_convs.7.0.bias', 'zero_convs.8.0.weight', 'zero_convs.8.0.bias', 'zero_convs.9.0.weight', 'zero_convs.9.0.bias', 'zero_convs.10.0.weight', 'zero_convs.10.0.bias', 'zero_convs.11.0.weight', 'zero_convs.11.0.bias', 'middle_block.0.in_layers.0.weight', 'middle_block.0.in_layers.0.bias', 'middle_block.0.in_layers.2.weight', 'middle_block.0.in_layers.2.bias', 'middle_block.0.emb_layers.1.weight', 'middle_block.0.emb_layers.1.bias', 'middle_block.0.out_layers.0.weight', 'middle_block.0.out_layers.0.bias', 'middle_block.0.out_layers.3.weight', 'middle_block.0.out_layers.3.bias', 'middle_block.1.norm.weight', 'middle_block.1.norm.bias', 'middle_block.1.proj_in.weight', 'middle_block.1.proj_in.bias', 'middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'middle_block.1.transformer_blocks.0.ff.net.2.weight', 'middle_block.1.transformer_blocks.0.ff.net.2.bias', 'middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'middle_block.1.transformer_blocks.0.norm1.weight', 'middle_block.1.transformer_blocks.0.norm1.bias', 'middle_block.1.transformer_blocks.0.norm2.weight', 'middle_block.1.transformer_blocks.0.norm2.bias', 'middle_block.1.transformer_blocks.0.norm3.weight', 'middle_block.1.transformer_blocks.0.norm3.bias', 'middle_block.1.proj_out.weight', 'middle_block.1.proj_out.bias', 'middle_block.2.in_layers.0.weight', 'middle_block.2.in_layers.0.bias', 'middle_block.2.in_layers.2.weight', 'middle_block.2.in_layers.2.bias', 'middle_block.2.emb_layers.1.weight', 'middle_block.2.emb_layers.1.bias', 'middle_block.2.out_layers.0.weight', 'middle_block.2.out_layers.0.bias', 'middle_block.2.out_layers.3.weight', 'middle_block.2.out_layers.3.bias', 'middle_block_out.0.weight', 'middle_block_out.0.bias']\n"
     ]
    }
   ],
   "source": [
    "print([k for k, v in model_scratch.control_model_list[0].named_parameters()])\n",
    "\n",
    "'input_blocks.0.0.weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54aba53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_scratch.load_state_dict(test_weight, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3ecd13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = model_scratch.control_model_list[1].load_state_dict(new_canny_checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed83c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 4, 3, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weight[\"input_blocks.0.0.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1b4273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_projection.weight\n",
      "cc_projection.bias\n",
      "control_model_list.0.time_embed.0.weight\n",
      "control_model_list.0.time_embed.0.bias\n",
      "control_model_list.0.time_embed.2.weight\n",
      "control_model_list.0.time_embed.2.bias\n",
      "control_model_list.0.input_blocks.0.0.weight\n",
      "control_model_list.0.input_blocks.0.0.bias\n",
      "control_model_list.0.input_blocks.1.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.1.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.1.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.1.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.1.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.1.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.1.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.1.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.1.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.1.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.1.1.norm.weight\n",
      "control_model_list.0.input_blocks.1.1.norm.bias\n",
      "control_model_list.0.input_blocks.1.1.proj_in.weight\n",
      "control_model_list.0.input_blocks.1.1.proj_in.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.input_blocks.1.1.proj_out.weight\n",
      "control_model_list.0.input_blocks.1.1.proj_out.bias\n",
      "control_model_list.0.input_blocks.2.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.2.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.2.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.2.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.2.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.2.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.2.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.2.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.2.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.2.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.2.1.norm.weight\n",
      "control_model_list.0.input_blocks.2.1.norm.bias\n",
      "control_model_list.0.input_blocks.2.1.proj_in.weight\n",
      "control_model_list.0.input_blocks.2.1.proj_in.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.input_blocks.2.1.proj_out.weight\n",
      "control_model_list.0.input_blocks.2.1.proj_out.bias\n",
      "control_model_list.0.input_blocks.3.0.op.weight\n",
      "control_model_list.0.input_blocks.3.0.op.bias\n",
      "control_model_list.0.input_blocks.4.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.4.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.4.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.4.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.4.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.4.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.4.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.4.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.4.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.4.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.4.0.skip_connection.weight\n",
      "control_model_list.0.input_blocks.4.0.skip_connection.bias\n",
      "control_model_list.0.input_blocks.4.1.norm.weight\n",
      "control_model_list.0.input_blocks.4.1.norm.bias\n",
      "control_model_list.0.input_blocks.4.1.proj_in.weight\n",
      "control_model_list.0.input_blocks.4.1.proj_in.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.input_blocks.4.1.proj_out.weight\n",
      "control_model_list.0.input_blocks.4.1.proj_out.bias\n",
      "control_model_list.0.input_blocks.5.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.5.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.5.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.5.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.5.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.5.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.5.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.5.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.5.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.5.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.5.1.norm.weight\n",
      "control_model_list.0.input_blocks.5.1.norm.bias\n",
      "control_model_list.0.input_blocks.5.1.proj_in.weight\n",
      "control_model_list.0.input_blocks.5.1.proj_in.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.input_blocks.5.1.proj_out.weight\n",
      "control_model_list.0.input_blocks.5.1.proj_out.bias\n",
      "control_model_list.0.input_blocks.6.0.op.weight\n",
      "control_model_list.0.input_blocks.6.0.op.bias\n",
      "control_model_list.0.input_blocks.7.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.7.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.7.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.7.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.7.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.7.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.7.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.7.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.7.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.7.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.7.0.skip_connection.weight\n",
      "control_model_list.0.input_blocks.7.0.skip_connection.bias\n",
      "control_model_list.0.input_blocks.7.1.norm.weight\n",
      "control_model_list.0.input_blocks.7.1.norm.bias\n",
      "control_model_list.0.input_blocks.7.1.proj_in.weight\n",
      "control_model_list.0.input_blocks.7.1.proj_in.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.input_blocks.7.1.proj_out.weight\n",
      "control_model_list.0.input_blocks.7.1.proj_out.bias\n",
      "control_model_list.0.input_blocks.8.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.8.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.8.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.8.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.8.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.8.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.8.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.8.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.8.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.8.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.8.1.norm.weight\n",
      "control_model_list.0.input_blocks.8.1.norm.bias\n",
      "control_model_list.0.input_blocks.8.1.proj_in.weight\n",
      "control_model_list.0.input_blocks.8.1.proj_in.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.input_blocks.8.1.proj_out.weight\n",
      "control_model_list.0.input_blocks.8.1.proj_out.bias\n",
      "control_model_list.0.input_blocks.9.0.op.weight\n",
      "control_model_list.0.input_blocks.9.0.op.bias\n",
      "control_model_list.0.input_blocks.10.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.10.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.10.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.10.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.10.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.10.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.10.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.10.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.10.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.10.0.out_layers.3.bias\n",
      "control_model_list.0.input_blocks.11.0.in_layers.0.weight\n",
      "control_model_list.0.input_blocks.11.0.in_layers.0.bias\n",
      "control_model_list.0.input_blocks.11.0.in_layers.2.weight\n",
      "control_model_list.0.input_blocks.11.0.in_layers.2.bias\n",
      "control_model_list.0.input_blocks.11.0.emb_layers.1.weight\n",
      "control_model_list.0.input_blocks.11.0.emb_layers.1.bias\n",
      "control_model_list.0.input_blocks.11.0.out_layers.0.weight\n",
      "control_model_list.0.input_blocks.11.0.out_layers.0.bias\n",
      "control_model_list.0.input_blocks.11.0.out_layers.3.weight\n",
      "control_model_list.0.input_blocks.11.0.out_layers.3.bias\n",
      "control_model_list.0.zero_convs.0.0.weight\n",
      "control_model_list.0.zero_convs.0.0.bias\n",
      "control_model_list.0.zero_convs.1.0.weight\n",
      "control_model_list.0.zero_convs.1.0.bias\n",
      "control_model_list.0.zero_convs.2.0.weight\n",
      "control_model_list.0.zero_convs.2.0.bias\n",
      "control_model_list.0.zero_convs.3.0.weight\n",
      "control_model_list.0.zero_convs.3.0.bias\n",
      "control_model_list.0.zero_convs.4.0.weight\n",
      "control_model_list.0.zero_convs.4.0.bias\n",
      "control_model_list.0.zero_convs.5.0.weight\n",
      "control_model_list.0.zero_convs.5.0.bias\n",
      "control_model_list.0.zero_convs.6.0.weight\n",
      "control_model_list.0.zero_convs.6.0.bias\n",
      "control_model_list.0.zero_convs.7.0.weight\n",
      "control_model_list.0.zero_convs.7.0.bias\n",
      "control_model_list.0.zero_convs.8.0.weight\n",
      "control_model_list.0.zero_convs.8.0.bias\n",
      "control_model_list.0.zero_convs.9.0.weight\n",
      "control_model_list.0.zero_convs.9.0.bias\n",
      "control_model_list.0.zero_convs.10.0.weight\n",
      "control_model_list.0.zero_convs.10.0.bias\n",
      "control_model_list.0.zero_convs.11.0.weight\n",
      "control_model_list.0.zero_convs.11.0.bias\n",
      "control_model_list.0.middle_block.0.in_layers.0.weight\n",
      "control_model_list.0.middle_block.0.in_layers.0.bias\n",
      "control_model_list.0.middle_block.0.in_layers.2.weight\n",
      "control_model_list.0.middle_block.0.in_layers.2.bias\n",
      "control_model_list.0.middle_block.0.emb_layers.1.weight\n",
      "control_model_list.0.middle_block.0.emb_layers.1.bias\n",
      "control_model_list.0.middle_block.0.out_layers.0.weight\n",
      "control_model_list.0.middle_block.0.out_layers.0.bias\n",
      "control_model_list.0.middle_block.0.out_layers.3.weight\n",
      "control_model_list.0.middle_block.0.out_layers.3.bias\n",
      "control_model_list.0.middle_block.1.norm.weight\n",
      "control_model_list.0.middle_block.1.norm.bias\n",
      "control_model_list.0.middle_block.1.proj_in.weight\n",
      "control_model_list.0.middle_block.1.proj_in.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.0.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.0.middle_block.1.proj_out.weight\n",
      "control_model_list.0.middle_block.1.proj_out.bias\n",
      "control_model_list.0.middle_block.2.in_layers.0.weight\n",
      "control_model_list.0.middle_block.2.in_layers.0.bias\n",
      "control_model_list.0.middle_block.2.in_layers.2.weight\n",
      "control_model_list.0.middle_block.2.in_layers.2.bias\n",
      "control_model_list.0.middle_block.2.emb_layers.1.weight\n",
      "control_model_list.0.middle_block.2.emb_layers.1.bias\n",
      "control_model_list.0.middle_block.2.out_layers.0.weight\n",
      "control_model_list.0.middle_block.2.out_layers.0.bias\n",
      "control_model_list.0.middle_block.2.out_layers.3.weight\n",
      "control_model_list.0.middle_block.2.out_layers.3.bias\n",
      "control_model_list.0.middle_block_out.0.weight\n",
      "control_model_list.0.middle_block_out.0.bias\n",
      "control_model_list.1.time_embed.0.weight\n",
      "control_model_list.1.time_embed.0.bias\n",
      "control_model_list.1.time_embed.2.weight\n",
      "control_model_list.1.time_embed.2.bias\n",
      "control_model_list.1.input_blocks.0.0.weight\n",
      "control_model_list.1.input_blocks.0.0.bias\n",
      "control_model_list.1.input_blocks.1.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.1.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.1.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.1.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.1.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.1.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.1.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.1.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.1.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.1.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.1.1.norm.weight\n",
      "control_model_list.1.input_blocks.1.1.norm.bias\n",
      "control_model_list.1.input_blocks.1.1.proj_in.weight\n",
      "control_model_list.1.input_blocks.1.1.proj_in.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.input_blocks.1.1.proj_out.weight\n",
      "control_model_list.1.input_blocks.1.1.proj_out.bias\n",
      "control_model_list.1.input_blocks.2.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.2.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.2.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.2.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.2.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.2.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.2.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.2.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.2.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.2.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.2.1.norm.weight\n",
      "control_model_list.1.input_blocks.2.1.norm.bias\n",
      "control_model_list.1.input_blocks.2.1.proj_in.weight\n",
      "control_model_list.1.input_blocks.2.1.proj_in.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.input_blocks.2.1.proj_out.weight\n",
      "control_model_list.1.input_blocks.2.1.proj_out.bias\n",
      "control_model_list.1.input_blocks.3.0.op.weight\n",
      "control_model_list.1.input_blocks.3.0.op.bias\n",
      "control_model_list.1.input_blocks.4.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.4.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.4.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.4.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.4.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.4.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.4.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.4.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.4.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.4.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.4.0.skip_connection.weight\n",
      "control_model_list.1.input_blocks.4.0.skip_connection.bias\n",
      "control_model_list.1.input_blocks.4.1.norm.weight\n",
      "control_model_list.1.input_blocks.4.1.norm.bias\n",
      "control_model_list.1.input_blocks.4.1.proj_in.weight\n",
      "control_model_list.1.input_blocks.4.1.proj_in.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.input_blocks.4.1.proj_out.weight\n",
      "control_model_list.1.input_blocks.4.1.proj_out.bias\n",
      "control_model_list.1.input_blocks.5.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.5.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.5.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.5.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.5.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.5.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.5.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.5.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.5.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.5.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.5.1.norm.weight\n",
      "control_model_list.1.input_blocks.5.1.norm.bias\n",
      "control_model_list.1.input_blocks.5.1.proj_in.weight\n",
      "control_model_list.1.input_blocks.5.1.proj_in.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.input_blocks.5.1.proj_out.weight\n",
      "control_model_list.1.input_blocks.5.1.proj_out.bias\n",
      "control_model_list.1.input_blocks.6.0.op.weight\n",
      "control_model_list.1.input_blocks.6.0.op.bias\n",
      "control_model_list.1.input_blocks.7.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.7.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.7.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.7.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.7.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.7.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.7.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.7.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.7.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.7.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.7.0.skip_connection.weight\n",
      "control_model_list.1.input_blocks.7.0.skip_connection.bias\n",
      "control_model_list.1.input_blocks.7.1.norm.weight\n",
      "control_model_list.1.input_blocks.7.1.norm.bias\n",
      "control_model_list.1.input_blocks.7.1.proj_in.weight\n",
      "control_model_list.1.input_blocks.7.1.proj_in.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.input_blocks.7.1.proj_out.weight\n",
      "control_model_list.1.input_blocks.7.1.proj_out.bias\n",
      "control_model_list.1.input_blocks.8.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.8.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.8.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.8.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.8.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.8.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.8.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.8.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.8.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.8.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.8.1.norm.weight\n",
      "control_model_list.1.input_blocks.8.1.norm.bias\n",
      "control_model_list.1.input_blocks.8.1.proj_in.weight\n",
      "control_model_list.1.input_blocks.8.1.proj_in.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.input_blocks.8.1.proj_out.weight\n",
      "control_model_list.1.input_blocks.8.1.proj_out.bias\n",
      "control_model_list.1.input_blocks.9.0.op.weight\n",
      "control_model_list.1.input_blocks.9.0.op.bias\n",
      "control_model_list.1.input_blocks.10.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.10.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.10.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.10.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.10.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.10.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.10.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.10.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.10.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.10.0.out_layers.3.bias\n",
      "control_model_list.1.input_blocks.11.0.in_layers.0.weight\n",
      "control_model_list.1.input_blocks.11.0.in_layers.0.bias\n",
      "control_model_list.1.input_blocks.11.0.in_layers.2.weight\n",
      "control_model_list.1.input_blocks.11.0.in_layers.2.bias\n",
      "control_model_list.1.input_blocks.11.0.emb_layers.1.weight\n",
      "control_model_list.1.input_blocks.11.0.emb_layers.1.bias\n",
      "control_model_list.1.input_blocks.11.0.out_layers.0.weight\n",
      "control_model_list.1.input_blocks.11.0.out_layers.0.bias\n",
      "control_model_list.1.input_blocks.11.0.out_layers.3.weight\n",
      "control_model_list.1.input_blocks.11.0.out_layers.3.bias\n",
      "control_model_list.1.zero_convs.0.0.weight\n",
      "control_model_list.1.zero_convs.0.0.bias\n",
      "control_model_list.1.zero_convs.1.0.weight\n",
      "control_model_list.1.zero_convs.1.0.bias\n",
      "control_model_list.1.zero_convs.2.0.weight\n",
      "control_model_list.1.zero_convs.2.0.bias\n",
      "control_model_list.1.zero_convs.3.0.weight\n",
      "control_model_list.1.zero_convs.3.0.bias\n",
      "control_model_list.1.zero_convs.4.0.weight\n",
      "control_model_list.1.zero_convs.4.0.bias\n",
      "control_model_list.1.zero_convs.5.0.weight\n",
      "control_model_list.1.zero_convs.5.0.bias\n",
      "control_model_list.1.zero_convs.6.0.weight\n",
      "control_model_list.1.zero_convs.6.0.bias\n",
      "control_model_list.1.zero_convs.7.0.weight\n",
      "control_model_list.1.zero_convs.7.0.bias\n",
      "control_model_list.1.zero_convs.8.0.weight\n",
      "control_model_list.1.zero_convs.8.0.bias\n",
      "control_model_list.1.zero_convs.9.0.weight\n",
      "control_model_list.1.zero_convs.9.0.bias\n",
      "control_model_list.1.zero_convs.10.0.weight\n",
      "control_model_list.1.zero_convs.10.0.bias\n",
      "control_model_list.1.zero_convs.11.0.weight\n",
      "control_model_list.1.zero_convs.11.0.bias\n",
      "control_model_list.1.input_hint_block.0.weight\n",
      "control_model_list.1.input_hint_block.0.bias\n",
      "control_model_list.1.input_hint_block.2.weight\n",
      "control_model_list.1.input_hint_block.2.bias\n",
      "control_model_list.1.input_hint_block.4.weight\n",
      "control_model_list.1.input_hint_block.4.bias\n",
      "control_model_list.1.input_hint_block.6.weight\n",
      "control_model_list.1.input_hint_block.6.bias\n",
      "control_model_list.1.input_hint_block.8.weight\n",
      "control_model_list.1.input_hint_block.8.bias\n",
      "control_model_list.1.input_hint_block.10.weight\n",
      "control_model_list.1.input_hint_block.10.bias\n",
      "control_model_list.1.input_hint_block.12.weight\n",
      "control_model_list.1.input_hint_block.12.bias\n",
      "control_model_list.1.input_hint_block.14.weight\n",
      "control_model_list.1.input_hint_block.14.bias\n",
      "control_model_list.1.middle_block.0.in_layers.0.weight\n",
      "control_model_list.1.middle_block.0.in_layers.0.bias\n",
      "control_model_list.1.middle_block.0.in_layers.2.weight\n",
      "control_model_list.1.middle_block.0.in_layers.2.bias\n",
      "control_model_list.1.middle_block.0.emb_layers.1.weight\n",
      "control_model_list.1.middle_block.0.emb_layers.1.bias\n",
      "control_model_list.1.middle_block.0.out_layers.0.weight\n",
      "control_model_list.1.middle_block.0.out_layers.0.bias\n",
      "control_model_list.1.middle_block.0.out_layers.3.weight\n",
      "control_model_list.1.middle_block.0.out_layers.3.bias\n",
      "control_model_list.1.middle_block.1.norm.weight\n",
      "control_model_list.1.middle_block.1.norm.bias\n",
      "control_model_list.1.middle_block.1.proj_in.weight\n",
      "control_model_list.1.middle_block.1.proj_in.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "control_model_list.1.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "control_model_list.1.middle_block.1.proj_out.weight\n",
      "control_model_list.1.middle_block.1.proj_out.bias\n",
      "control_model_list.1.middle_block.2.in_layers.0.weight\n",
      "control_model_list.1.middle_block.2.in_layers.0.bias\n",
      "control_model_list.1.middle_block.2.in_layers.2.weight\n",
      "control_model_list.1.middle_block.2.in_layers.2.bias\n",
      "control_model_list.1.middle_block.2.emb_layers.1.weight\n",
      "control_model_list.1.middle_block.2.emb_layers.1.bias\n",
      "control_model_list.1.middle_block.2.out_layers.0.weight\n",
      "control_model_list.1.middle_block.2.out_layers.0.bias\n",
      "control_model_list.1.middle_block.2.out_layers.3.weight\n",
      "control_model_list.1.middle_block.2.out_layers.3.bias\n",
      "control_model_list.1.middle_block_out.0.weight\n",
      "control_model_list.1.middle_block_out.0.bias\n",
      "===\n",
      "model_ema.decay\n",
      "model_ema.num_updates\n",
      "cond_stage_model.transformer.text_model.embeddings.position_ids\n"
     ]
    }
   ],
   "source": [
    "for key in result.missing_keys:\n",
    "    print(key)\n",
    "print(\"===\")\n",
    "for key in result.unexpected_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9e5dc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-3.2378e-04]],\n",
       "\n",
       "         [[ 5.4660e-03]],\n",
       "\n",
       "         [[-5.6703e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.9892e-03]],\n",
       "\n",
       "         [[ 4.2112e-03]],\n",
       "\n",
       "         [[-5.7546e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9025e-03]],\n",
       "\n",
       "         [[-6.9791e-03]],\n",
       "\n",
       "         [[ 4.2329e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.1953e-03]],\n",
       "\n",
       "         [[ 5.1083e-03]],\n",
       "\n",
       "         [[ 6.1575e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 3.2860e-05]],\n",
       "\n",
       "         [[ 8.7645e-04]],\n",
       "\n",
       "         [[ 2.5143e-04]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.7085e-03]],\n",
       "\n",
       "         [[-4.1409e-04]],\n",
       "\n",
       "         [[-4.8830e-03]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-4.1807e-03]],\n",
       "\n",
       "         [[-3.6767e-03]],\n",
       "\n",
       "         [[ 6.2936e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0634e-03]],\n",
       "\n",
       "         [[-5.2150e-03]],\n",
       "\n",
       "         [[ 4.4617e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2845e-03]],\n",
       "\n",
       "         [[ 6.9236e-03]],\n",
       "\n",
       "         [[-5.4860e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.9793e-03]],\n",
       "\n",
       "         [[ 1.2769e-02]],\n",
       "\n",
       "         [[-7.1989e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.2720e-03]],\n",
       "\n",
       "         [[-8.9656e-03]],\n",
       "\n",
       "         [[-2.6518e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.8615e-03]],\n",
       "\n",
       "         [[-1.0064e-02]],\n",
       "\n",
       "         [[-2.7386e-04]]]], requires_grad=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scratch.control_model_list[1].zero_convs[0][0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e04b8139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n"
     ]
    }
   ],
   "source": [
    "for key in result2.missing_keys:\n",
    "    print(key)\n",
    "print(\"===\")\n",
    "for key in result2.unexpected_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1762218",
   "metadata": {},
   "source": [
    "# load function 짜기\n",
    "- 처음에는 stable diffusion origin\n",
    "- 이후에는 각각의 controlNet model들에 대하여\n",
    "    - 다만, zero123는 8 -> 4 시에 2개 channel씩 average 해주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cd754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiControlNet: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "ename": "ConfigAttributeError",
     "evalue": "Missing key itmes\n    full_key: pre_control.path.itmes\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigAttributeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_scratch \u001b[38;5;241m=\u001b[39m instantiate_from_config(control_scratch_cfg\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_scratch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pre_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_control_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_scratch_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_control\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/code/3DAnything/zero123/ldm/models/diffusion/controlnet.py:226\u001b[0m, in \u001b[0;36mMultiControlNet.load_pre_control\u001b[0;34m(self, pre_control_cfg)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pre_control\u001b[39m(\u001b[38;5;28mself\u001b[39m, pre_control_cfg: DictConfig):\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpre_control_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitmes\u001b[49m():\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m control model weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/dictconfig.py:355\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[1;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_and_raise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfigAttributeError\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[0;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[43mformat_and_raise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcause\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtype_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[1;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[0;32m--> 899\u001b[0m \u001b[43m_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_MARKER_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/dictconfig.py:442\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[0;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_impl\u001b[39m(\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: DictKeyType, default_value: Any, validate_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    440\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_child\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrow_on_missing_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_key\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ConfigAttributeError, ConfigKeyError):\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_MARKER_:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/basecontainer.py:73\u001b[0m, in \u001b[0;36mBaseContainer._get_child\u001b[0;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_child\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     throw_on_missing_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Optional[Node], List[Optional[Node]]]:\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrow_on_missing_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrow_on_missing_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrow_on_missing_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrow_on_missing_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, UnionNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_special(child):\n\u001b[1;32m     81\u001b[0m         value \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39m_value()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/omegaconf/dictconfig.py:480\u001b[0m, in \u001b[0;36mDictConfig._get_node\u001b[0;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m throw_on_missing_key:\n\u001b[0;32m--> 480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m throw_on_missing_value \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39m_is_missing():\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConfigAttributeError\u001b[0m: Missing key itmes\n    full_key: pre_control.path.itmes\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "model_scratch = instantiate_from_config(control_scratch_cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "437a9483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sd control model weights\n",
      "sd Control model weights: _IncompatibleKeys(missing_keys=['cc_projection.weight', 'cc_projection.bias', 'control_model_list.0.time_embed.0.weight', 'control_model_list.0.time_embed.0.bias', 'control_model_list.0.time_embed.2.weight', 'control_model_list.0.time_embed.2.bias', 'control_model_list.0.input_blocks.0.0.weight', 'control_model_list.0.input_blocks.0.0.bias', 'control_model_list.0.input_blocks.1.0.in_layers.0.weight', 'control_model_list.0.input_blocks.1.0.in_layers.0.bias', 'control_model_list.0.input_blocks.1.0.in_layers.2.weight', 'control_model_list.0.input_blocks.1.0.in_layers.2.bias', 'control_model_list.0.input_blocks.1.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.1.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.1.0.out_layers.0.weight', 'control_model_list.0.input_blocks.1.0.out_layers.0.bias', 'control_model_list.0.input_blocks.1.0.out_layers.3.weight', 'control_model_list.0.input_blocks.1.0.out_layers.3.bias', 'control_model_list.0.input_blocks.1.1.norm.weight', 'control_model_list.0.input_blocks.1.1.norm.bias', 'control_model_list.0.input_blocks.1.1.proj_in.weight', 'control_model_list.0.input_blocks.1.1.proj_in.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.input_blocks.1.1.proj_out.weight', 'control_model_list.0.input_blocks.1.1.proj_out.bias', 'control_model_list.0.input_blocks.2.0.in_layers.0.weight', 'control_model_list.0.input_blocks.2.0.in_layers.0.bias', 'control_model_list.0.input_blocks.2.0.in_layers.2.weight', 'control_model_list.0.input_blocks.2.0.in_layers.2.bias', 'control_model_list.0.input_blocks.2.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.2.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.2.0.out_layers.0.weight', 'control_model_list.0.input_blocks.2.0.out_layers.0.bias', 'control_model_list.0.input_blocks.2.0.out_layers.3.weight', 'control_model_list.0.input_blocks.2.0.out_layers.3.bias', 'control_model_list.0.input_blocks.2.1.norm.weight', 'control_model_list.0.input_blocks.2.1.norm.bias', 'control_model_list.0.input_blocks.2.1.proj_in.weight', 'control_model_list.0.input_blocks.2.1.proj_in.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.input_blocks.2.1.proj_out.weight', 'control_model_list.0.input_blocks.2.1.proj_out.bias', 'control_model_list.0.input_blocks.3.0.op.weight', 'control_model_list.0.input_blocks.3.0.op.bias', 'control_model_list.0.input_blocks.4.0.in_layers.0.weight', 'control_model_list.0.input_blocks.4.0.in_layers.0.bias', 'control_model_list.0.input_blocks.4.0.in_layers.2.weight', 'control_model_list.0.input_blocks.4.0.in_layers.2.bias', 'control_model_list.0.input_blocks.4.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.4.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.4.0.out_layers.0.weight', 'control_model_list.0.input_blocks.4.0.out_layers.0.bias', 'control_model_list.0.input_blocks.4.0.out_layers.3.weight', 'control_model_list.0.input_blocks.4.0.out_layers.3.bias', 'control_model_list.0.input_blocks.4.0.skip_connection.weight', 'control_model_list.0.input_blocks.4.0.skip_connection.bias', 'control_model_list.0.input_blocks.4.1.norm.weight', 'control_model_list.0.input_blocks.4.1.norm.bias', 'control_model_list.0.input_blocks.4.1.proj_in.weight', 'control_model_list.0.input_blocks.4.1.proj_in.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.input_blocks.4.1.proj_out.weight', 'control_model_list.0.input_blocks.4.1.proj_out.bias', 'control_model_list.0.input_blocks.5.0.in_layers.0.weight', 'control_model_list.0.input_blocks.5.0.in_layers.0.bias', 'control_model_list.0.input_blocks.5.0.in_layers.2.weight', 'control_model_list.0.input_blocks.5.0.in_layers.2.bias', 'control_model_list.0.input_blocks.5.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.5.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.5.0.out_layers.0.weight', 'control_model_list.0.input_blocks.5.0.out_layers.0.bias', 'control_model_list.0.input_blocks.5.0.out_layers.3.weight', 'control_model_list.0.input_blocks.5.0.out_layers.3.bias', 'control_model_list.0.input_blocks.5.1.norm.weight', 'control_model_list.0.input_blocks.5.1.norm.bias', 'control_model_list.0.input_blocks.5.1.proj_in.weight', 'control_model_list.0.input_blocks.5.1.proj_in.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.input_blocks.5.1.proj_out.weight', 'control_model_list.0.input_blocks.5.1.proj_out.bias', 'control_model_list.0.input_blocks.6.0.op.weight', 'control_model_list.0.input_blocks.6.0.op.bias', 'control_model_list.0.input_blocks.7.0.in_layers.0.weight', 'control_model_list.0.input_blocks.7.0.in_layers.0.bias', 'control_model_list.0.input_blocks.7.0.in_layers.2.weight', 'control_model_list.0.input_blocks.7.0.in_layers.2.bias', 'control_model_list.0.input_blocks.7.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.7.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.7.0.out_layers.0.weight', 'control_model_list.0.input_blocks.7.0.out_layers.0.bias', 'control_model_list.0.input_blocks.7.0.out_layers.3.weight', 'control_model_list.0.input_blocks.7.0.out_layers.3.bias', 'control_model_list.0.input_blocks.7.0.skip_connection.weight', 'control_model_list.0.input_blocks.7.0.skip_connection.bias', 'control_model_list.0.input_blocks.7.1.norm.weight', 'control_model_list.0.input_blocks.7.1.norm.bias', 'control_model_list.0.input_blocks.7.1.proj_in.weight', 'control_model_list.0.input_blocks.7.1.proj_in.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.input_blocks.7.1.proj_out.weight', 'control_model_list.0.input_blocks.7.1.proj_out.bias', 'control_model_list.0.input_blocks.8.0.in_layers.0.weight', 'control_model_list.0.input_blocks.8.0.in_layers.0.bias', 'control_model_list.0.input_blocks.8.0.in_layers.2.weight', 'control_model_list.0.input_blocks.8.0.in_layers.2.bias', 'control_model_list.0.input_blocks.8.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.8.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.8.0.out_layers.0.weight', 'control_model_list.0.input_blocks.8.0.out_layers.0.bias', 'control_model_list.0.input_blocks.8.0.out_layers.3.weight', 'control_model_list.0.input_blocks.8.0.out_layers.3.bias', 'control_model_list.0.input_blocks.8.1.norm.weight', 'control_model_list.0.input_blocks.8.1.norm.bias', 'control_model_list.0.input_blocks.8.1.proj_in.weight', 'control_model_list.0.input_blocks.8.1.proj_in.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.input_blocks.8.1.proj_out.weight', 'control_model_list.0.input_blocks.8.1.proj_out.bias', 'control_model_list.0.input_blocks.9.0.op.weight', 'control_model_list.0.input_blocks.9.0.op.bias', 'control_model_list.0.input_blocks.10.0.in_layers.0.weight', 'control_model_list.0.input_blocks.10.0.in_layers.0.bias', 'control_model_list.0.input_blocks.10.0.in_layers.2.weight', 'control_model_list.0.input_blocks.10.0.in_layers.2.bias', 'control_model_list.0.input_blocks.10.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.10.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.10.0.out_layers.0.weight', 'control_model_list.0.input_blocks.10.0.out_layers.0.bias', 'control_model_list.0.input_blocks.10.0.out_layers.3.weight', 'control_model_list.0.input_blocks.10.0.out_layers.3.bias', 'control_model_list.0.input_blocks.11.0.in_layers.0.weight', 'control_model_list.0.input_blocks.11.0.in_layers.0.bias', 'control_model_list.0.input_blocks.11.0.in_layers.2.weight', 'control_model_list.0.input_blocks.11.0.in_layers.2.bias', 'control_model_list.0.input_blocks.11.0.emb_layers.1.weight', 'control_model_list.0.input_blocks.11.0.emb_layers.1.bias', 'control_model_list.0.input_blocks.11.0.out_layers.0.weight', 'control_model_list.0.input_blocks.11.0.out_layers.0.bias', 'control_model_list.0.input_blocks.11.0.out_layers.3.weight', 'control_model_list.0.input_blocks.11.0.out_layers.3.bias', 'control_model_list.0.zero_convs.0.0.weight', 'control_model_list.0.zero_convs.0.0.bias', 'control_model_list.0.zero_convs.1.0.weight', 'control_model_list.0.zero_convs.1.0.bias', 'control_model_list.0.zero_convs.2.0.weight', 'control_model_list.0.zero_convs.2.0.bias', 'control_model_list.0.zero_convs.3.0.weight', 'control_model_list.0.zero_convs.3.0.bias', 'control_model_list.0.zero_convs.4.0.weight', 'control_model_list.0.zero_convs.4.0.bias', 'control_model_list.0.zero_convs.5.0.weight', 'control_model_list.0.zero_convs.5.0.bias', 'control_model_list.0.zero_convs.6.0.weight', 'control_model_list.0.zero_convs.6.0.bias', 'control_model_list.0.zero_convs.7.0.weight', 'control_model_list.0.zero_convs.7.0.bias', 'control_model_list.0.zero_convs.8.0.weight', 'control_model_list.0.zero_convs.8.0.bias', 'control_model_list.0.zero_convs.9.0.weight', 'control_model_list.0.zero_convs.9.0.bias', 'control_model_list.0.zero_convs.10.0.weight', 'control_model_list.0.zero_convs.10.0.bias', 'control_model_list.0.zero_convs.11.0.weight', 'control_model_list.0.zero_convs.11.0.bias', 'control_model_list.0.middle_block.0.in_layers.0.weight', 'control_model_list.0.middle_block.0.in_layers.0.bias', 'control_model_list.0.middle_block.0.in_layers.2.weight', 'control_model_list.0.middle_block.0.in_layers.2.bias', 'control_model_list.0.middle_block.0.emb_layers.1.weight', 'control_model_list.0.middle_block.0.emb_layers.1.bias', 'control_model_list.0.middle_block.0.out_layers.0.weight', 'control_model_list.0.middle_block.0.out_layers.0.bias', 'control_model_list.0.middle_block.0.out_layers.3.weight', 'control_model_list.0.middle_block.0.out_layers.3.bias', 'control_model_list.0.middle_block.1.norm.weight', 'control_model_list.0.middle_block.1.norm.bias', 'control_model_list.0.middle_block.1.proj_in.weight', 'control_model_list.0.middle_block.1.proj_in.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model_list.0.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model_list.0.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model_list.0.middle_block.1.proj_out.weight', 'control_model_list.0.middle_block.1.proj_out.bias', 'control_model_list.0.middle_block.2.in_layers.0.weight', 'control_model_list.0.middle_block.2.in_layers.0.bias', 'control_model_list.0.middle_block.2.in_layers.2.weight', 'control_model_list.0.middle_block.2.in_layers.2.bias', 'control_model_list.0.middle_block.2.emb_layers.1.weight', 'control_model_list.0.middle_block.2.emb_layers.1.bias', 'control_model_list.0.middle_block.2.out_layers.0.weight', 'control_model_list.0.middle_block.2.out_layers.0.bias', 'control_model_list.0.middle_block.2.out_layers.3.weight', 'control_model_list.0.middle_block.2.out_layers.3.bias', 'control_model_list.0.middle_block_out.0.weight', 'control_model_list.0.middle_block_out.0.bias', 'control_model_list.1.time_embed.0.weight', 'control_model_list.1.time_embed.0.bias', 'control_model_list.1.time_embed.2.weight', 'control_model_list.1.time_embed.2.bias', 'control_model_list.1.input_blocks.0.0.weight', 'control_model_list.1.input_blocks.0.0.bias', 'control_model_list.1.input_blocks.1.0.in_layers.0.weight', 'control_model_list.1.input_blocks.1.0.in_layers.0.bias', 'control_model_list.1.input_blocks.1.0.in_layers.2.weight', 'control_model_list.1.input_blocks.1.0.in_layers.2.bias', 'control_model_list.1.input_blocks.1.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.1.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.1.0.out_layers.0.weight', 'control_model_list.1.input_blocks.1.0.out_layers.0.bias', 'control_model_list.1.input_blocks.1.0.out_layers.3.weight', 'control_model_list.1.input_blocks.1.0.out_layers.3.bias', 'control_model_list.1.input_blocks.1.1.norm.weight', 'control_model_list.1.input_blocks.1.1.norm.bias', 'control_model_list.1.input_blocks.1.1.proj_in.weight', 'control_model_list.1.input_blocks.1.1.proj_in.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.input_blocks.1.1.proj_out.weight', 'control_model_list.1.input_blocks.1.1.proj_out.bias', 'control_model_list.1.input_blocks.2.0.in_layers.0.weight', 'control_model_list.1.input_blocks.2.0.in_layers.0.bias', 'control_model_list.1.input_blocks.2.0.in_layers.2.weight', 'control_model_list.1.input_blocks.2.0.in_layers.2.bias', 'control_model_list.1.input_blocks.2.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.2.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.2.0.out_layers.0.weight', 'control_model_list.1.input_blocks.2.0.out_layers.0.bias', 'control_model_list.1.input_blocks.2.0.out_layers.3.weight', 'control_model_list.1.input_blocks.2.0.out_layers.3.bias', 'control_model_list.1.input_blocks.2.1.norm.weight', 'control_model_list.1.input_blocks.2.1.norm.bias', 'control_model_list.1.input_blocks.2.1.proj_in.weight', 'control_model_list.1.input_blocks.2.1.proj_in.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.input_blocks.2.1.proj_out.weight', 'control_model_list.1.input_blocks.2.1.proj_out.bias', 'control_model_list.1.input_blocks.3.0.op.weight', 'control_model_list.1.input_blocks.3.0.op.bias', 'control_model_list.1.input_blocks.4.0.in_layers.0.weight', 'control_model_list.1.input_blocks.4.0.in_layers.0.bias', 'control_model_list.1.input_blocks.4.0.in_layers.2.weight', 'control_model_list.1.input_blocks.4.0.in_layers.2.bias', 'control_model_list.1.input_blocks.4.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.4.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.4.0.out_layers.0.weight', 'control_model_list.1.input_blocks.4.0.out_layers.0.bias', 'control_model_list.1.input_blocks.4.0.out_layers.3.weight', 'control_model_list.1.input_blocks.4.0.out_layers.3.bias', 'control_model_list.1.input_blocks.4.0.skip_connection.weight', 'control_model_list.1.input_blocks.4.0.skip_connection.bias', 'control_model_list.1.input_blocks.4.1.norm.weight', 'control_model_list.1.input_blocks.4.1.norm.bias', 'control_model_list.1.input_blocks.4.1.proj_in.weight', 'control_model_list.1.input_blocks.4.1.proj_in.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.input_blocks.4.1.proj_out.weight', 'control_model_list.1.input_blocks.4.1.proj_out.bias', 'control_model_list.1.input_blocks.5.0.in_layers.0.weight', 'control_model_list.1.input_blocks.5.0.in_layers.0.bias', 'control_model_list.1.input_blocks.5.0.in_layers.2.weight', 'control_model_list.1.input_blocks.5.0.in_layers.2.bias', 'control_model_list.1.input_blocks.5.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.5.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.5.0.out_layers.0.weight', 'control_model_list.1.input_blocks.5.0.out_layers.0.bias', 'control_model_list.1.input_blocks.5.0.out_layers.3.weight', 'control_model_list.1.input_blocks.5.0.out_layers.3.bias', 'control_model_list.1.input_blocks.5.1.norm.weight', 'control_model_list.1.input_blocks.5.1.norm.bias', 'control_model_list.1.input_blocks.5.1.proj_in.weight', 'control_model_list.1.input_blocks.5.1.proj_in.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.input_blocks.5.1.proj_out.weight', 'control_model_list.1.input_blocks.5.1.proj_out.bias', 'control_model_list.1.input_blocks.6.0.op.weight', 'control_model_list.1.input_blocks.6.0.op.bias', 'control_model_list.1.input_blocks.7.0.in_layers.0.weight', 'control_model_list.1.input_blocks.7.0.in_layers.0.bias', 'control_model_list.1.input_blocks.7.0.in_layers.2.weight', 'control_model_list.1.input_blocks.7.0.in_layers.2.bias', 'control_model_list.1.input_blocks.7.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.7.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.7.0.out_layers.0.weight', 'control_model_list.1.input_blocks.7.0.out_layers.0.bias', 'control_model_list.1.input_blocks.7.0.out_layers.3.weight', 'control_model_list.1.input_blocks.7.0.out_layers.3.bias', 'control_model_list.1.input_blocks.7.0.skip_connection.weight', 'control_model_list.1.input_blocks.7.0.skip_connection.bias', 'control_model_list.1.input_blocks.7.1.norm.weight', 'control_model_list.1.input_blocks.7.1.norm.bias', 'control_model_list.1.input_blocks.7.1.proj_in.weight', 'control_model_list.1.input_blocks.7.1.proj_in.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.input_blocks.7.1.proj_out.weight', 'control_model_list.1.input_blocks.7.1.proj_out.bias', 'control_model_list.1.input_blocks.8.0.in_layers.0.weight', 'control_model_list.1.input_blocks.8.0.in_layers.0.bias', 'control_model_list.1.input_blocks.8.0.in_layers.2.weight', 'control_model_list.1.input_blocks.8.0.in_layers.2.bias', 'control_model_list.1.input_blocks.8.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.8.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.8.0.out_layers.0.weight', 'control_model_list.1.input_blocks.8.0.out_layers.0.bias', 'control_model_list.1.input_blocks.8.0.out_layers.3.weight', 'control_model_list.1.input_blocks.8.0.out_layers.3.bias', 'control_model_list.1.input_blocks.8.1.norm.weight', 'control_model_list.1.input_blocks.8.1.norm.bias', 'control_model_list.1.input_blocks.8.1.proj_in.weight', 'control_model_list.1.input_blocks.8.1.proj_in.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.input_blocks.8.1.proj_out.weight', 'control_model_list.1.input_blocks.8.1.proj_out.bias', 'control_model_list.1.input_blocks.9.0.op.weight', 'control_model_list.1.input_blocks.9.0.op.bias', 'control_model_list.1.input_blocks.10.0.in_layers.0.weight', 'control_model_list.1.input_blocks.10.0.in_layers.0.bias', 'control_model_list.1.input_blocks.10.0.in_layers.2.weight', 'control_model_list.1.input_blocks.10.0.in_layers.2.bias', 'control_model_list.1.input_blocks.10.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.10.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.10.0.out_layers.0.weight', 'control_model_list.1.input_blocks.10.0.out_layers.0.bias', 'control_model_list.1.input_blocks.10.0.out_layers.3.weight', 'control_model_list.1.input_blocks.10.0.out_layers.3.bias', 'control_model_list.1.input_blocks.11.0.in_layers.0.weight', 'control_model_list.1.input_blocks.11.0.in_layers.0.bias', 'control_model_list.1.input_blocks.11.0.in_layers.2.weight', 'control_model_list.1.input_blocks.11.0.in_layers.2.bias', 'control_model_list.1.input_blocks.11.0.emb_layers.1.weight', 'control_model_list.1.input_blocks.11.0.emb_layers.1.bias', 'control_model_list.1.input_blocks.11.0.out_layers.0.weight', 'control_model_list.1.input_blocks.11.0.out_layers.0.bias', 'control_model_list.1.input_blocks.11.0.out_layers.3.weight', 'control_model_list.1.input_blocks.11.0.out_layers.3.bias', 'control_model_list.1.zero_convs.0.0.weight', 'control_model_list.1.zero_convs.0.0.bias', 'control_model_list.1.zero_convs.1.0.weight', 'control_model_list.1.zero_convs.1.0.bias', 'control_model_list.1.zero_convs.2.0.weight', 'control_model_list.1.zero_convs.2.0.bias', 'control_model_list.1.zero_convs.3.0.weight', 'control_model_list.1.zero_convs.3.0.bias', 'control_model_list.1.zero_convs.4.0.weight', 'control_model_list.1.zero_convs.4.0.bias', 'control_model_list.1.zero_convs.5.0.weight', 'control_model_list.1.zero_convs.5.0.bias', 'control_model_list.1.zero_convs.6.0.weight', 'control_model_list.1.zero_convs.6.0.bias', 'control_model_list.1.zero_convs.7.0.weight', 'control_model_list.1.zero_convs.7.0.bias', 'control_model_list.1.zero_convs.8.0.weight', 'control_model_list.1.zero_convs.8.0.bias', 'control_model_list.1.zero_convs.9.0.weight', 'control_model_list.1.zero_convs.9.0.bias', 'control_model_list.1.zero_convs.10.0.weight', 'control_model_list.1.zero_convs.10.0.bias', 'control_model_list.1.zero_convs.11.0.weight', 'control_model_list.1.zero_convs.11.0.bias', 'control_model_list.1.input_hint_block.0.weight', 'control_model_list.1.input_hint_block.0.bias', 'control_model_list.1.input_hint_block.2.weight', 'control_model_list.1.input_hint_block.2.bias', 'control_model_list.1.input_hint_block.4.weight', 'control_model_list.1.input_hint_block.4.bias', 'control_model_list.1.input_hint_block.6.weight', 'control_model_list.1.input_hint_block.6.bias', 'control_model_list.1.input_hint_block.8.weight', 'control_model_list.1.input_hint_block.8.bias', 'control_model_list.1.input_hint_block.10.weight', 'control_model_list.1.input_hint_block.10.bias', 'control_model_list.1.input_hint_block.12.weight', 'control_model_list.1.input_hint_block.12.bias', 'control_model_list.1.input_hint_block.14.weight', 'control_model_list.1.input_hint_block.14.bias', 'control_model_list.1.middle_block.0.in_layers.0.weight', 'control_model_list.1.middle_block.0.in_layers.0.bias', 'control_model_list.1.middle_block.0.in_layers.2.weight', 'control_model_list.1.middle_block.0.in_layers.2.bias', 'control_model_list.1.middle_block.0.emb_layers.1.weight', 'control_model_list.1.middle_block.0.emb_layers.1.bias', 'control_model_list.1.middle_block.0.out_layers.0.weight', 'control_model_list.1.middle_block.0.out_layers.0.bias', 'control_model_list.1.middle_block.0.out_layers.3.weight', 'control_model_list.1.middle_block.0.out_layers.3.bias', 'control_model_list.1.middle_block.1.norm.weight', 'control_model_list.1.middle_block.1.norm.bias', 'control_model_list.1.middle_block.1.proj_in.weight', 'control_model_list.1.middle_block.1.proj_in.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model_list.1.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model_list.1.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model_list.1.middle_block.1.proj_out.weight', 'control_model_list.1.middle_block.1.proj_out.bias', 'control_model_list.1.middle_block.2.in_layers.0.weight', 'control_model_list.1.middle_block.2.in_layers.0.bias', 'control_model_list.1.middle_block.2.in_layers.2.weight', 'control_model_list.1.middle_block.2.in_layers.2.bias', 'control_model_list.1.middle_block.2.emb_layers.1.weight', 'control_model_list.1.middle_block.2.emb_layers.1.bias', 'control_model_list.1.middle_block.2.out_layers.0.weight', 'control_model_list.1.middle_block.2.out_layers.0.bias', 'control_model_list.1.middle_block.2.out_layers.3.weight', 'control_model_list.1.middle_block.2.out_layers.3.bias', 'control_model_list.1.middle_block_out.0.weight', 'control_model_list.1.middle_block_out.0.bias'], unexpected_keys=['model_ema.decay', 'model_ema.num_updates', 'cond_stage_model.transformer.text_model.embeddings.position_ids'])\n",
      "Loading pose control model weights\n",
      "pose Control model weights: _IncompatibleKeys(missing_keys=['zero_convs.0.0.weight', 'zero_convs.0.0.bias', 'zero_convs.1.0.weight', 'zero_convs.1.0.bias', 'zero_convs.2.0.weight', 'zero_convs.2.0.bias', 'zero_convs.3.0.weight', 'zero_convs.3.0.bias', 'zero_convs.4.0.weight', 'zero_convs.4.0.bias', 'zero_convs.5.0.weight', 'zero_convs.5.0.bias', 'zero_convs.6.0.weight', 'zero_convs.6.0.bias', 'zero_convs.7.0.weight', 'zero_convs.7.0.bias', 'zero_convs.8.0.weight', 'zero_convs.8.0.bias', 'zero_convs.9.0.weight', 'zero_convs.9.0.bias', 'zero_convs.10.0.weight', 'zero_convs.10.0.bias', 'zero_convs.11.0.weight', 'zero_convs.11.0.bias', 'middle_block_out.0.weight', 'middle_block_out.0.bias'], unexpected_keys=[])\n",
      "Loading canny control model weights\n",
      "canny Control model weights: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_scratch.load_pre_control(pre_control_cfg=control_scratch_cfg.pre_control, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "78acd5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "test = nn.ModuleList()\n",
    "test.append(\n",
    "   nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) \n",
    ")\n",
    "test.append(\n",
    "   nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "242c9db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7ff04c5c3ed0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d61bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/workspace/code/3DAnything/zero123/configs/canny-edge_variant_i2t_multimodal_v3_512.yaml\"\n",
    "test_cfg = OmegaConf.load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a275c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiModalControlNetV3: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.53 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "new_model = instantiate_from_config(test_cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87d9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sd control model weights\n",
      "sd Control model weights: _IncompatibleKeys(missing_keys=['zero_convs.0.0.weight', 'zero_convs.0.0.bias', 'zero_convs.1.0.weight', 'zero_convs.1.0.bias', 'zero_convs.2.0.weight', 'zero_convs.2.0.bias', 'zero_convs.3.0.weight', 'zero_convs.3.0.bias', 'zero_convs.4.0.weight', 'zero_convs.4.0.bias', 'zero_convs.5.0.weight', 'zero_convs.5.0.bias', 'zero_convs.6.0.weight', 'zero_convs.6.0.bias', 'zero_convs.7.0.weight', 'zero_convs.7.0.bias', 'zero_convs.8.0.weight', 'zero_convs.8.0.bias', 'zero_convs.9.0.weight', 'zero_convs.9.0.bias', 'zero_convs.10.0.weight', 'zero_convs.10.0.bias', 'zero_convs.11.0.weight', 'zero_convs.11.0.bias', 'middle_block_out.0.weight', 'middle_block_out.0.bias'], unexpected_keys=[])\n",
      "Loading pose control model weights\n",
      "pose Control model weights: _IncompatibleKeys(missing_keys=['cond_stage_model.txt.transformer.text_model.embeddings.token_embedding.weight', 'cond_stage_model.txt.transformer.text_model.embeddings.position_embedding.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.0.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.1.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.2.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.3.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.4.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.5.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.6.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.7.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.8.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.9.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.10.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.layer_norm1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.layer_norm1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.mlp.fc1.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.mlp.fc1.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.mlp.fc2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.mlp.fc2.bias', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.layer_norm2.weight', 'cond_stage_model.txt.transformer.text_model.encoder.layers.11.layer_norm2.bias', 'cond_stage_model.txt.transformer.text_model.final_layer_norm.weight', 'cond_stage_model.txt.transformer.text_model.final_layer_norm.bias', 'cond_stage_model.image.model.positional_embedding', 'cond_stage_model.image.model.text_projection', 'cond_stage_model.image.model.logit_scale', 'cond_stage_model.image.model.visual.class_embedding', 'cond_stage_model.image.model.visual.positional_embedding', 'cond_stage_model.image.model.visual.proj', 'cond_stage_model.image.model.visual.conv1.weight', 'cond_stage_model.image.model.visual.ln_pre.weight', 'cond_stage_model.image.model.visual.ln_pre.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.0.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.0.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.0.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.0.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.0.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.0.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.0.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.0.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.1.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.1.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.1.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.1.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.1.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.1.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.1.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.1.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.2.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.2.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.2.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.2.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.2.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.2.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.2.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.2.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.3.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.3.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.3.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.3.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.3.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.3.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.3.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.3.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.4.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.4.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.4.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.4.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.4.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.4.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.4.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.4.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.5.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.5.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.5.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.5.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.5.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.5.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.5.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.5.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.6.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.6.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.6.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.6.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.6.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.6.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.6.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.6.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.7.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.7.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.7.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.7.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.7.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.7.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.7.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.7.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.8.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.8.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.8.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.8.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.8.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.8.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.8.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.8.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.9.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.9.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.9.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.9.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.9.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.9.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.9.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.9.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.10.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.10.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.10.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.10.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.10.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.10.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.10.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.10.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.11.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.11.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.11.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.11.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.11.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.11.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.11.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.11.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.12.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.12.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.12.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.12.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.12.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.12.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.12.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.12.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.13.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.13.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.13.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.13.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.13.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.13.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.13.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.13.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.14.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.14.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.14.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.14.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.14.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.14.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.14.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.14.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.15.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.15.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.15.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.15.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.15.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.15.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.15.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.15.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.16.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.16.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.16.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.16.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.16.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.16.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.16.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.16.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.17.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.17.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.17.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.17.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.17.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.17.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.17.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.17.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.18.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.18.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.18.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.18.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.18.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.18.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.18.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.18.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.19.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.19.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.19.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.19.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.19.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.19.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.19.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.19.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.20.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.20.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.20.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.20.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.20.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.20.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.20.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.20.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.21.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.21.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.21.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.21.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.21.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.21.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.21.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.21.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.22.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.22.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.22.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.22.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.22.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.22.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.22.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.22.ln_2.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.23.attn.in_proj_weight', 'cond_stage_model.image.model.visual.transformer.resblocks.23.attn.in_proj_bias', 'cond_stage_model.image.model.visual.transformer.resblocks.23.attn.out_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.23.attn.out_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.23.ln_1.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.23.ln_1.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'cond_stage_model.image.model.visual.transformer.resblocks.23.ln_2.weight', 'cond_stage_model.image.model.visual.transformer.resblocks.23.ln_2.bias', 'cond_stage_model.image.model.visual.ln_post.weight', 'cond_stage_model.image.model.visual.ln_post.bias', 'cond_stage_model.image.model.token_embedding.weight', 'cond_stage_model.image.model.ln_final.weight', 'cond_stage_model.image.model.ln_final.bias', 'control_model.time_embed.0.weight', 'control_model.time_embed.0.bias', 'control_model.time_embed.2.weight', 'control_model.time_embed.2.bias', 'control_model.input_blocks.0.0.weight', 'control_model.input_blocks.0.0.bias', 'control_model.input_blocks.1.0.in_layers.0.weight', 'control_model.input_blocks.1.0.in_layers.0.bias', 'control_model.input_blocks.1.0.in_layers.2.weight', 'control_model.input_blocks.1.0.in_layers.2.bias', 'control_model.input_blocks.1.0.emb_layers.1.weight', 'control_model.input_blocks.1.0.emb_layers.1.bias', 'control_model.input_blocks.1.0.out_layers.0.weight', 'control_model.input_blocks.1.0.out_layers.0.bias', 'control_model.input_blocks.1.0.out_layers.3.weight', 'control_model.input_blocks.1.0.out_layers.3.bias', 'control_model.input_blocks.1.1.norm.weight', 'control_model.input_blocks.1.1.norm.bias', 'control_model.input_blocks.1.1.proj_in.weight', 'control_model.input_blocks.1.1.proj_in.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.1.1.proj_out.weight', 'control_model.input_blocks.1.1.proj_out.bias', 'control_model.input_blocks.2.0.in_layers.0.weight', 'control_model.input_blocks.2.0.in_layers.0.bias', 'control_model.input_blocks.2.0.in_layers.2.weight', 'control_model.input_blocks.2.0.in_layers.2.bias', 'control_model.input_blocks.2.0.emb_layers.1.weight', 'control_model.input_blocks.2.0.emb_layers.1.bias', 'control_model.input_blocks.2.0.out_layers.0.weight', 'control_model.input_blocks.2.0.out_layers.0.bias', 'control_model.input_blocks.2.0.out_layers.3.weight', 'control_model.input_blocks.2.0.out_layers.3.bias', 'control_model.input_blocks.2.1.norm.weight', 'control_model.input_blocks.2.1.norm.bias', 'control_model.input_blocks.2.1.proj_in.weight', 'control_model.input_blocks.2.1.proj_in.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.2.1.proj_out.weight', 'control_model.input_blocks.2.1.proj_out.bias', 'control_model.input_blocks.3.0.op.weight', 'control_model.input_blocks.3.0.op.bias', 'control_model.input_blocks.4.0.in_layers.0.weight', 'control_model.input_blocks.4.0.in_layers.0.bias', 'control_model.input_blocks.4.0.in_layers.2.weight', 'control_model.input_blocks.4.0.in_layers.2.bias', 'control_model.input_blocks.4.0.emb_layers.1.weight', 'control_model.input_blocks.4.0.emb_layers.1.bias', 'control_model.input_blocks.4.0.out_layers.0.weight', 'control_model.input_blocks.4.0.out_layers.0.bias', 'control_model.input_blocks.4.0.out_layers.3.weight', 'control_model.input_blocks.4.0.out_layers.3.bias', 'control_model.input_blocks.4.0.skip_connection.weight', 'control_model.input_blocks.4.0.skip_connection.bias', 'control_model.input_blocks.4.1.norm.weight', 'control_model.input_blocks.4.1.norm.bias', 'control_model.input_blocks.4.1.proj_in.weight', 'control_model.input_blocks.4.1.proj_in.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.4.1.proj_out.weight', 'control_model.input_blocks.4.1.proj_out.bias', 'control_model.input_blocks.5.0.in_layers.0.weight', 'control_model.input_blocks.5.0.in_layers.0.bias', 'control_model.input_blocks.5.0.in_layers.2.weight', 'control_model.input_blocks.5.0.in_layers.2.bias', 'control_model.input_blocks.5.0.emb_layers.1.weight', 'control_model.input_blocks.5.0.emb_layers.1.bias', 'control_model.input_blocks.5.0.out_layers.0.weight', 'control_model.input_blocks.5.0.out_layers.0.bias', 'control_model.input_blocks.5.0.out_layers.3.weight', 'control_model.input_blocks.5.0.out_layers.3.bias', 'control_model.input_blocks.5.1.norm.weight', 'control_model.input_blocks.5.1.norm.bias', 'control_model.input_blocks.5.1.proj_in.weight', 'control_model.input_blocks.5.1.proj_in.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.5.1.proj_out.weight', 'control_model.input_blocks.5.1.proj_out.bias', 'control_model.input_blocks.6.0.op.weight', 'control_model.input_blocks.6.0.op.bias', 'control_model.input_blocks.7.0.in_layers.0.weight', 'control_model.input_blocks.7.0.in_layers.0.bias', 'control_model.input_blocks.7.0.in_layers.2.weight', 'control_model.input_blocks.7.0.in_layers.2.bias', 'control_model.input_blocks.7.0.emb_layers.1.weight', 'control_model.input_blocks.7.0.emb_layers.1.bias', 'control_model.input_blocks.7.0.out_layers.0.weight', 'control_model.input_blocks.7.0.out_layers.0.bias', 'control_model.input_blocks.7.0.out_layers.3.weight', 'control_model.input_blocks.7.0.out_layers.3.bias', 'control_model.input_blocks.7.0.skip_connection.weight', 'control_model.input_blocks.7.0.skip_connection.bias', 'control_model.input_blocks.7.1.norm.weight', 'control_model.input_blocks.7.1.norm.bias', 'control_model.input_blocks.7.1.proj_in.weight', 'control_model.input_blocks.7.1.proj_in.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.7.1.proj_out.weight', 'control_model.input_blocks.7.1.proj_out.bias', 'control_model.input_blocks.8.0.in_layers.0.weight', 'control_model.input_blocks.8.0.in_layers.0.bias', 'control_model.input_blocks.8.0.in_layers.2.weight', 'control_model.input_blocks.8.0.in_layers.2.bias', 'control_model.input_blocks.8.0.emb_layers.1.weight', 'control_model.input_blocks.8.0.emb_layers.1.bias', 'control_model.input_blocks.8.0.out_layers.0.weight', 'control_model.input_blocks.8.0.out_layers.0.bias', 'control_model.input_blocks.8.0.out_layers.3.weight', 'control_model.input_blocks.8.0.out_layers.3.bias', 'control_model.input_blocks.8.1.norm.weight', 'control_model.input_blocks.8.1.norm.bias', 'control_model.input_blocks.8.1.proj_in.weight', 'control_model.input_blocks.8.1.proj_in.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.8.1.proj_out.weight', 'control_model.input_blocks.8.1.proj_out.bias', 'control_model.input_blocks.9.0.op.weight', 'control_model.input_blocks.9.0.op.bias', 'control_model.input_blocks.10.0.in_layers.0.weight', 'control_model.input_blocks.10.0.in_layers.0.bias', 'control_model.input_blocks.10.0.in_layers.2.weight', 'control_model.input_blocks.10.0.in_layers.2.bias', 'control_model.input_blocks.10.0.emb_layers.1.weight', 'control_model.input_blocks.10.0.emb_layers.1.bias', 'control_model.input_blocks.10.0.out_layers.0.weight', 'control_model.input_blocks.10.0.out_layers.0.bias', 'control_model.input_blocks.10.0.out_layers.3.weight', 'control_model.input_blocks.10.0.out_layers.3.bias', 'control_model.input_blocks.11.0.in_layers.0.weight', 'control_model.input_blocks.11.0.in_layers.0.bias', 'control_model.input_blocks.11.0.in_layers.2.weight', 'control_model.input_blocks.11.0.in_layers.2.bias', 'control_model.input_blocks.11.0.emb_layers.1.weight', 'control_model.input_blocks.11.0.emb_layers.1.bias', 'control_model.input_blocks.11.0.out_layers.0.weight', 'control_model.input_blocks.11.0.out_layers.0.bias', 'control_model.input_blocks.11.0.out_layers.3.weight', 'control_model.input_blocks.11.0.out_layers.3.bias', 'control_model.zero_convs.0.0.weight', 'control_model.zero_convs.0.0.bias', 'control_model.zero_convs.1.0.weight', 'control_model.zero_convs.1.0.bias', 'control_model.zero_convs.2.0.weight', 'control_model.zero_convs.2.0.bias', 'control_model.zero_convs.3.0.weight', 'control_model.zero_convs.3.0.bias', 'control_model.zero_convs.4.0.weight', 'control_model.zero_convs.4.0.bias', 'control_model.zero_convs.5.0.weight', 'control_model.zero_convs.5.0.bias', 'control_model.zero_convs.6.0.weight', 'control_model.zero_convs.6.0.bias', 'control_model.zero_convs.7.0.weight', 'control_model.zero_convs.7.0.bias', 'control_model.zero_convs.8.0.weight', 'control_model.zero_convs.8.0.bias', 'control_model.zero_convs.9.0.weight', 'control_model.zero_convs.9.0.bias', 'control_model.zero_convs.10.0.weight', 'control_model.zero_convs.10.0.bias', 'control_model.zero_convs.11.0.weight', 'control_model.zero_convs.11.0.bias', 'control_model.middle_block.0.in_layers.0.weight', 'control_model.middle_block.0.in_layers.0.bias', 'control_model.middle_block.0.in_layers.2.weight', 'control_model.middle_block.0.in_layers.2.bias', 'control_model.middle_block.0.emb_layers.1.weight', 'control_model.middle_block.0.emb_layers.1.bias', 'control_model.middle_block.0.out_layers.0.weight', 'control_model.middle_block.0.out_layers.0.bias', 'control_model.middle_block.0.out_layers.3.weight', 'control_model.middle_block.0.out_layers.3.bias', 'control_model.middle_block.1.norm.weight', 'control_model.middle_block.1.norm.bias', 'control_model.middle_block.1.proj_in.weight', 'control_model.middle_block.1.proj_in.bias', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model.middle_block.1.proj_out.weight', 'control_model.middle_block.1.proj_out.bias', 'control_model.middle_block.2.in_layers.0.weight', 'control_model.middle_block.2.in_layers.0.bias', 'control_model.middle_block.2.in_layers.2.weight', 'control_model.middle_block.2.in_layers.2.bias', 'control_model.middle_block.2.emb_layers.1.weight', 'control_model.middle_block.2.emb_layers.1.bias', 'control_model.middle_block.2.out_layers.0.weight', 'control_model.middle_block.2.out_layers.0.bias', 'control_model.middle_block.2.out_layers.3.weight', 'control_model.middle_block.2.out_layers.3.bias', 'control_model.middle_block_out.0.weight', 'control_model.middle_block_out.0.bias'], unexpected_keys=['model_ema.decay', 'model_ema.num_updates', 'model_ema.diffusion_modeltime_embed0weight', 'model_ema.diffusion_modeltime_embed0bias', 'model_ema.diffusion_modeltime_embed2weight', 'model_ema.diffusion_modeltime_embed2bias', 'model_ema.diffusion_modelinput_blocks00weight', 'model_ema.diffusion_modelinput_blocks00bias', 'model_ema.diffusion_modelinput_blocks10in_layers0weight', 'model_ema.diffusion_modelinput_blocks10in_layers0bias', 'model_ema.diffusion_modelinput_blocks10in_layers2weight', 'model_ema.diffusion_modelinput_blocks10in_layers2bias', 'model_ema.diffusion_modelinput_blocks10emb_layers1weight', 'model_ema.diffusion_modelinput_blocks10emb_layers1bias', 'model_ema.diffusion_modelinput_blocks10out_layers0weight', 'model_ema.diffusion_modelinput_blocks10out_layers0bias', 'model_ema.diffusion_modelinput_blocks10out_layers3weight', 'model_ema.diffusion_modelinput_blocks10out_layers3bias', 'model_ema.diffusion_modelinput_blocks11normweight', 'model_ema.diffusion_modelinput_blocks11normbias', 'model_ema.diffusion_modelinput_blocks11proj_inweight', 'model_ema.diffusion_modelinput_blocks11proj_inbias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks11proj_outweight', 'model_ema.diffusion_modelinput_blocks11proj_outbias', 'model_ema.diffusion_modelinput_blocks20in_layers0weight', 'model_ema.diffusion_modelinput_blocks20in_layers0bias', 'model_ema.diffusion_modelinput_blocks20in_layers2weight', 'model_ema.diffusion_modelinput_blocks20in_layers2bias', 'model_ema.diffusion_modelinput_blocks20emb_layers1weight', 'model_ema.diffusion_modelinput_blocks20emb_layers1bias', 'model_ema.diffusion_modelinput_blocks20out_layers0weight', 'model_ema.diffusion_modelinput_blocks20out_layers0bias', 'model_ema.diffusion_modelinput_blocks20out_layers3weight', 'model_ema.diffusion_modelinput_blocks20out_layers3bias', 'model_ema.diffusion_modelinput_blocks21normweight', 'model_ema.diffusion_modelinput_blocks21normbias', 'model_ema.diffusion_modelinput_blocks21proj_inweight', 'model_ema.diffusion_modelinput_blocks21proj_inbias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks21proj_outweight', 'model_ema.diffusion_modelinput_blocks21proj_outbias', 'model_ema.diffusion_modelinput_blocks30opweight', 'model_ema.diffusion_modelinput_blocks30opbias', 'model_ema.diffusion_modelinput_blocks40in_layers0weight', 'model_ema.diffusion_modelinput_blocks40in_layers0bias', 'model_ema.diffusion_modelinput_blocks40in_layers2weight', 'model_ema.diffusion_modelinput_blocks40in_layers2bias', 'model_ema.diffusion_modelinput_blocks40emb_layers1weight', 'model_ema.diffusion_modelinput_blocks40emb_layers1bias', 'model_ema.diffusion_modelinput_blocks40out_layers0weight', 'model_ema.diffusion_modelinput_blocks40out_layers0bias', 'model_ema.diffusion_modelinput_blocks40out_layers3weight', 'model_ema.diffusion_modelinput_blocks40out_layers3bias', 'model_ema.diffusion_modelinput_blocks40skip_connectionweight', 'model_ema.diffusion_modelinput_blocks40skip_connectionbias', 'model_ema.diffusion_modelinput_blocks41normweight', 'model_ema.diffusion_modelinput_blocks41normbias', 'model_ema.diffusion_modelinput_blocks41proj_inweight', 'model_ema.diffusion_modelinput_blocks41proj_inbias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks41proj_outweight', 'model_ema.diffusion_modelinput_blocks41proj_outbias', 'model_ema.diffusion_modelinput_blocks50in_layers0weight', 'model_ema.diffusion_modelinput_blocks50in_layers0bias', 'model_ema.diffusion_modelinput_blocks50in_layers2weight', 'model_ema.diffusion_modelinput_blocks50in_layers2bias', 'model_ema.diffusion_modelinput_blocks50emb_layers1weight', 'model_ema.diffusion_modelinput_blocks50emb_layers1bias', 'model_ema.diffusion_modelinput_blocks50out_layers0weight', 'model_ema.diffusion_modelinput_blocks50out_layers0bias', 'model_ema.diffusion_modelinput_blocks50out_layers3weight', 'model_ema.diffusion_modelinput_blocks50out_layers3bias', 'model_ema.diffusion_modelinput_blocks51normweight', 'model_ema.diffusion_modelinput_blocks51normbias', 'model_ema.diffusion_modelinput_blocks51proj_inweight', 'model_ema.diffusion_modelinput_blocks51proj_inbias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks51proj_outweight', 'model_ema.diffusion_modelinput_blocks51proj_outbias', 'model_ema.diffusion_modelinput_blocks60opweight', 'model_ema.diffusion_modelinput_blocks60opbias', 'model_ema.diffusion_modelinput_blocks70in_layers0weight', 'model_ema.diffusion_modelinput_blocks70in_layers0bias', 'model_ema.diffusion_modelinput_blocks70in_layers2weight', 'model_ema.diffusion_modelinput_blocks70in_layers2bias', 'model_ema.diffusion_modelinput_blocks70emb_layers1weight', 'model_ema.diffusion_modelinput_blocks70emb_layers1bias', 'model_ema.diffusion_modelinput_blocks70out_layers0weight', 'model_ema.diffusion_modelinput_blocks70out_layers0bias', 'model_ema.diffusion_modelinput_blocks70out_layers3weight', 'model_ema.diffusion_modelinput_blocks70out_layers3bias', 'model_ema.diffusion_modelinput_blocks70skip_connectionweight', 'model_ema.diffusion_modelinput_blocks70skip_connectionbias', 'model_ema.diffusion_modelinput_blocks71normweight', 'model_ema.diffusion_modelinput_blocks71normbias', 'model_ema.diffusion_modelinput_blocks71proj_inweight', 'model_ema.diffusion_modelinput_blocks71proj_inbias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks71proj_outweight', 'model_ema.diffusion_modelinput_blocks71proj_outbias', 'model_ema.diffusion_modelinput_blocks80in_layers0weight', 'model_ema.diffusion_modelinput_blocks80in_layers0bias', 'model_ema.diffusion_modelinput_blocks80in_layers2weight', 'model_ema.diffusion_modelinput_blocks80in_layers2bias', 'model_ema.diffusion_modelinput_blocks80emb_layers1weight', 'model_ema.diffusion_modelinput_blocks80emb_layers1bias', 'model_ema.diffusion_modelinput_blocks80out_layers0weight', 'model_ema.diffusion_modelinput_blocks80out_layers0bias', 'model_ema.diffusion_modelinput_blocks80out_layers3weight', 'model_ema.diffusion_modelinput_blocks80out_layers3bias', 'model_ema.diffusion_modelinput_blocks81normweight', 'model_ema.diffusion_modelinput_blocks81normbias', 'model_ema.diffusion_modelinput_blocks81proj_inweight', 'model_ema.diffusion_modelinput_blocks81proj_inbias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks81proj_outweight', 'model_ema.diffusion_modelinput_blocks81proj_outbias', 'model_ema.diffusion_modelinput_blocks90opweight', 'model_ema.diffusion_modelinput_blocks90opbias', 'model_ema.diffusion_modelinput_blocks100in_layers0weight', 'model_ema.diffusion_modelinput_blocks100in_layers0bias', 'model_ema.diffusion_modelinput_blocks100in_layers2weight', 'model_ema.diffusion_modelinput_blocks100in_layers2bias', 'model_ema.diffusion_modelinput_blocks100emb_layers1weight', 'model_ema.diffusion_modelinput_blocks100emb_layers1bias', 'model_ema.diffusion_modelinput_blocks100out_layers0weight', 'model_ema.diffusion_modelinput_blocks100out_layers0bias', 'model_ema.diffusion_modelinput_blocks100out_layers3weight', 'model_ema.diffusion_modelinput_blocks100out_layers3bias', 'model_ema.diffusion_modelinput_blocks110in_layers0weight', 'model_ema.diffusion_modelinput_blocks110in_layers0bias', 'model_ema.diffusion_modelinput_blocks110in_layers2weight', 'model_ema.diffusion_modelinput_blocks110in_layers2bias', 'model_ema.diffusion_modelinput_blocks110emb_layers1weight', 'model_ema.diffusion_modelinput_blocks110emb_layers1bias', 'model_ema.diffusion_modelinput_blocks110out_layers0weight', 'model_ema.diffusion_modelinput_blocks110out_layers0bias', 'model_ema.diffusion_modelinput_blocks110out_layers3weight', 'model_ema.diffusion_modelinput_blocks110out_layers3bias', 'model_ema.diffusion_modelmiddle_block0in_layers0weight', 'model_ema.diffusion_modelmiddle_block0in_layers0bias', 'model_ema.diffusion_modelmiddle_block0in_layers2weight', 'model_ema.diffusion_modelmiddle_block0in_layers2bias', 'model_ema.diffusion_modelmiddle_block0emb_layers1weight', 'model_ema.diffusion_modelmiddle_block0emb_layers1bias', 'model_ema.diffusion_modelmiddle_block0out_layers0weight', 'model_ema.diffusion_modelmiddle_block0out_layers0bias', 'model_ema.diffusion_modelmiddle_block0out_layers3weight', 'model_ema.diffusion_modelmiddle_block0out_layers3bias', 'model_ema.diffusion_modelmiddle_block1normweight', 'model_ema.diffusion_modelmiddle_block1normbias', 'model_ema.diffusion_modelmiddle_block1proj_inweight', 'model_ema.diffusion_modelmiddle_block1proj_inbias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm1weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm1bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm2weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm2bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm3weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm3bias', 'model_ema.diffusion_modelmiddle_block1proj_outweight', 'model_ema.diffusion_modelmiddle_block1proj_outbias', 'model_ema.diffusion_modelmiddle_block2in_layers0weight', 'model_ema.diffusion_modelmiddle_block2in_layers0bias', 'model_ema.diffusion_modelmiddle_block2in_layers2weight', 'model_ema.diffusion_modelmiddle_block2in_layers2bias', 'model_ema.diffusion_modelmiddle_block2emb_layers1weight', 'model_ema.diffusion_modelmiddle_block2emb_layers1bias', 'model_ema.diffusion_modelmiddle_block2out_layers0weight', 'model_ema.diffusion_modelmiddle_block2out_layers0bias', 'model_ema.diffusion_modelmiddle_block2out_layers3weight', 'model_ema.diffusion_modelmiddle_block2out_layers3bias', 'model_ema.diffusion_modeloutput_blocks00in_layers0weight', 'model_ema.diffusion_modeloutput_blocks00in_layers0bias', 'model_ema.diffusion_modeloutput_blocks00in_layers2weight', 'model_ema.diffusion_modeloutput_blocks00in_layers2bias', 'model_ema.diffusion_modeloutput_blocks00emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks00emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks00out_layers0weight', 'model_ema.diffusion_modeloutput_blocks00out_layers0bias', 'model_ema.diffusion_modeloutput_blocks00out_layers3weight', 'model_ema.diffusion_modeloutput_blocks00out_layers3bias', 'model_ema.diffusion_modeloutput_blocks00skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks00skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks10in_layers0weight', 'model_ema.diffusion_modeloutput_blocks10in_layers0bias', 'model_ema.diffusion_modeloutput_blocks10in_layers2weight', 'model_ema.diffusion_modeloutput_blocks10in_layers2bias', 'model_ema.diffusion_modeloutput_blocks10emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks10emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks10out_layers0weight', 'model_ema.diffusion_modeloutput_blocks10out_layers0bias', 'model_ema.diffusion_modeloutput_blocks10out_layers3weight', 'model_ema.diffusion_modeloutput_blocks10out_layers3bias', 'model_ema.diffusion_modeloutput_blocks10skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks10skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks20in_layers0weight', 'model_ema.diffusion_modeloutput_blocks20in_layers0bias', 'model_ema.diffusion_modeloutput_blocks20in_layers2weight', 'model_ema.diffusion_modeloutput_blocks20in_layers2bias', 'model_ema.diffusion_modeloutput_blocks20emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks20emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks20out_layers0weight', 'model_ema.diffusion_modeloutput_blocks20out_layers0bias', 'model_ema.diffusion_modeloutput_blocks20out_layers3weight', 'model_ema.diffusion_modeloutput_blocks20out_layers3bias', 'model_ema.diffusion_modeloutput_blocks20skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks20skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks21convweight', 'model_ema.diffusion_modeloutput_blocks21convbias', 'model_ema.diffusion_modeloutput_blocks30in_layers0weight', 'model_ema.diffusion_modeloutput_blocks30in_layers0bias', 'model_ema.diffusion_modeloutput_blocks30in_layers2weight', 'model_ema.diffusion_modeloutput_blocks30in_layers2bias', 'model_ema.diffusion_modeloutput_blocks30emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks30emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks30out_layers0weight', 'model_ema.diffusion_modeloutput_blocks30out_layers0bias', 'model_ema.diffusion_modeloutput_blocks30out_layers3weight', 'model_ema.diffusion_modeloutput_blocks30out_layers3bias', 'model_ema.diffusion_modeloutput_blocks30skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks30skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks31normweight', 'model_ema.diffusion_modeloutput_blocks31normbias', 'model_ema.diffusion_modeloutput_blocks31proj_inweight', 'model_ema.diffusion_modeloutput_blocks31proj_inbias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks31proj_outweight', 'model_ema.diffusion_modeloutput_blocks31proj_outbias', 'model_ema.diffusion_modeloutput_blocks40in_layers0weight', 'model_ema.diffusion_modeloutput_blocks40in_layers0bias', 'model_ema.diffusion_modeloutput_blocks40in_layers2weight', 'model_ema.diffusion_modeloutput_blocks40in_layers2bias', 'model_ema.diffusion_modeloutput_blocks40emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks40emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks40out_layers0weight', 'model_ema.diffusion_modeloutput_blocks40out_layers0bias', 'model_ema.diffusion_modeloutput_blocks40out_layers3weight', 'model_ema.diffusion_modeloutput_blocks40out_layers3bias', 'model_ema.diffusion_modeloutput_blocks40skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks40skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks41normweight', 'model_ema.diffusion_modeloutput_blocks41normbias', 'model_ema.diffusion_modeloutput_blocks41proj_inweight', 'model_ema.diffusion_modeloutput_blocks41proj_inbias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks41proj_outweight', 'model_ema.diffusion_modeloutput_blocks41proj_outbias', 'model_ema.diffusion_modeloutput_blocks50in_layers0weight', 'model_ema.diffusion_modeloutput_blocks50in_layers0bias', 'model_ema.diffusion_modeloutput_blocks50in_layers2weight', 'model_ema.diffusion_modeloutput_blocks50in_layers2bias', 'model_ema.diffusion_modeloutput_blocks50emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks50emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks50out_layers0weight', 'model_ema.diffusion_modeloutput_blocks50out_layers0bias', 'model_ema.diffusion_modeloutput_blocks50out_layers3weight', 'model_ema.diffusion_modeloutput_blocks50out_layers3bias', 'model_ema.diffusion_modeloutput_blocks50skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks50skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks51normweight', 'model_ema.diffusion_modeloutput_blocks51normbias', 'model_ema.diffusion_modeloutput_blocks51proj_inweight', 'model_ema.diffusion_modeloutput_blocks51proj_inbias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks51proj_outweight', 'model_ema.diffusion_modeloutput_blocks51proj_outbias', 'model_ema.diffusion_modeloutput_blocks52convweight', 'model_ema.diffusion_modeloutput_blocks52convbias', 'model_ema.diffusion_modeloutput_blocks60in_layers0weight', 'model_ema.diffusion_modeloutput_blocks60in_layers0bias', 'model_ema.diffusion_modeloutput_blocks60in_layers2weight', 'model_ema.diffusion_modeloutput_blocks60in_layers2bias', 'model_ema.diffusion_modeloutput_blocks60emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks60emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks60out_layers0weight', 'model_ema.diffusion_modeloutput_blocks60out_layers0bias', 'model_ema.diffusion_modeloutput_blocks60out_layers3weight', 'model_ema.diffusion_modeloutput_blocks60out_layers3bias', 'model_ema.diffusion_modeloutput_blocks60skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks60skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks61normweight', 'model_ema.diffusion_modeloutput_blocks61normbias', 'model_ema.diffusion_modeloutput_blocks61proj_inweight', 'model_ema.diffusion_modeloutput_blocks61proj_inbias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks61proj_outweight', 'model_ema.diffusion_modeloutput_blocks61proj_outbias', 'model_ema.diffusion_modeloutput_blocks70in_layers0weight', 'model_ema.diffusion_modeloutput_blocks70in_layers0bias', 'model_ema.diffusion_modeloutput_blocks70in_layers2weight', 'model_ema.diffusion_modeloutput_blocks70in_layers2bias', 'model_ema.diffusion_modeloutput_blocks70emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks70emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks70out_layers0weight', 'model_ema.diffusion_modeloutput_blocks70out_layers0bias', 'model_ema.diffusion_modeloutput_blocks70out_layers3weight', 'model_ema.diffusion_modeloutput_blocks70out_layers3bias', 'model_ema.diffusion_modeloutput_blocks70skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks70skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks71normweight', 'model_ema.diffusion_modeloutput_blocks71normbias', 'model_ema.diffusion_modeloutput_blocks71proj_inweight', 'model_ema.diffusion_modeloutput_blocks71proj_inbias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks71proj_outweight', 'model_ema.diffusion_modeloutput_blocks71proj_outbias', 'model_ema.diffusion_modeloutput_blocks80in_layers0weight', 'model_ema.diffusion_modeloutput_blocks80in_layers0bias', 'model_ema.diffusion_modeloutput_blocks80in_layers2weight', 'model_ema.diffusion_modeloutput_blocks80in_layers2bias', 'model_ema.diffusion_modeloutput_blocks80emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks80emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks80out_layers0weight', 'model_ema.diffusion_modeloutput_blocks80out_layers0bias', 'model_ema.diffusion_modeloutput_blocks80out_layers3weight', 'model_ema.diffusion_modeloutput_blocks80out_layers3bias', 'model_ema.diffusion_modeloutput_blocks80skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks80skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks81normweight', 'model_ema.diffusion_modeloutput_blocks81normbias', 'model_ema.diffusion_modeloutput_blocks81proj_inweight', 'model_ema.diffusion_modeloutput_blocks81proj_inbias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks81proj_outweight', 'model_ema.diffusion_modeloutput_blocks81proj_outbias', 'model_ema.diffusion_modeloutput_blocks82convweight', 'model_ema.diffusion_modeloutput_blocks82convbias', 'model_ema.diffusion_modeloutput_blocks90in_layers0weight', 'model_ema.diffusion_modeloutput_blocks90in_layers0bias', 'model_ema.diffusion_modeloutput_blocks90in_layers2weight', 'model_ema.diffusion_modeloutput_blocks90in_layers2bias', 'model_ema.diffusion_modeloutput_blocks90emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks90emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks90out_layers0weight', 'model_ema.diffusion_modeloutput_blocks90out_layers0bias', 'model_ema.diffusion_modeloutput_blocks90out_layers3weight', 'model_ema.diffusion_modeloutput_blocks90out_layers3bias', 'model_ema.diffusion_modeloutput_blocks90skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks90skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks91normweight', 'model_ema.diffusion_modeloutput_blocks91normbias', 'model_ema.diffusion_modeloutput_blocks91proj_inweight', 'model_ema.diffusion_modeloutput_blocks91proj_inbias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks91proj_outweight', 'model_ema.diffusion_modeloutput_blocks91proj_outbias', 'model_ema.diffusion_modeloutput_blocks100in_layers0weight', 'model_ema.diffusion_modeloutput_blocks100in_layers0bias', 'model_ema.diffusion_modeloutput_blocks100in_layers2weight', 'model_ema.diffusion_modeloutput_blocks100in_layers2bias', 'model_ema.diffusion_modeloutput_blocks100emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks100emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks100out_layers0weight', 'model_ema.diffusion_modeloutput_blocks100out_layers0bias', 'model_ema.diffusion_modeloutput_blocks100out_layers3weight', 'model_ema.diffusion_modeloutput_blocks100out_layers3bias', 'model_ema.diffusion_modeloutput_blocks100skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks100skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks101normweight', 'model_ema.diffusion_modeloutput_blocks101normbias', 'model_ema.diffusion_modeloutput_blocks101proj_inweight', 'model_ema.diffusion_modeloutput_blocks101proj_inbias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks101proj_outweight', 'model_ema.diffusion_modeloutput_blocks101proj_outbias', 'model_ema.diffusion_modeloutput_blocks110in_layers0weight', 'model_ema.diffusion_modeloutput_blocks110in_layers0bias', 'model_ema.diffusion_modeloutput_blocks110in_layers2weight', 'model_ema.diffusion_modeloutput_blocks110in_layers2bias', 'model_ema.diffusion_modeloutput_blocks110emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks110emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks110out_layers0weight', 'model_ema.diffusion_modeloutput_blocks110out_layers0bias', 'model_ema.diffusion_modeloutput_blocks110out_layers3weight', 'model_ema.diffusion_modeloutput_blocks110out_layers3bias', 'model_ema.diffusion_modeloutput_blocks110skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks110skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks111normweight', 'model_ema.diffusion_modeloutput_blocks111normbias', 'model_ema.diffusion_modeloutput_blocks111proj_inweight', 'model_ema.diffusion_modeloutput_blocks111proj_inbias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks111proj_outweight', 'model_ema.diffusion_modeloutput_blocks111proj_outbias', 'model_ema.diffusion_modelout0weight', 'model_ema.diffusion_modelout0bias', 'model_ema.diffusion_modelout2weight', 'model_ema.diffusion_modelout2bias', 'cond_stage_model.model.positional_embedding', 'cond_stage_model.model.text_projection', 'cond_stage_model.model.logit_scale', 'cond_stage_model.model.visual.class_embedding', 'cond_stage_model.model.visual.positional_embedding', 'cond_stage_model.model.visual.proj', 'cond_stage_model.model.visual.conv1.weight', 'cond_stage_model.model.visual.ln_pre.weight', 'cond_stage_model.model.visual.ln_pre.bias', 'cond_stage_model.model.visual.transformer.resblocks.0.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.0.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.0.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.0.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.0.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.0.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.0.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.0.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.1.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.1.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.1.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.1.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.1.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.1.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.1.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.1.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.2.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.2.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.2.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.2.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.2.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.2.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.2.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.2.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.3.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.3.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.3.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.3.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.3.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.3.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.3.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.3.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.4.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.4.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.4.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.4.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.4.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.4.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.4.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.4.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.5.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.5.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.5.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.5.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.5.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.5.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.5.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.5.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.6.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.6.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.6.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.6.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.6.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.6.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.6.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.6.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.7.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.7.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.7.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.7.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.7.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.7.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.7.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.7.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.8.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.8.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.8.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.8.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.8.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.8.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.8.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.8.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.9.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.9.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.9.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.9.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.9.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.9.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.9.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.9.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.10.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.10.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.10.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.10.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.10.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.10.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.10.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.10.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.11.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.11.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.11.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.11.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.11.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.11.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.11.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.11.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.12.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.12.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.12.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.12.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.12.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.12.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.12.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.12.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.13.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.13.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.13.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.13.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.13.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.13.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.13.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.13.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.14.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.14.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.14.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.14.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.14.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.14.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.14.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.14.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.15.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.15.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.15.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.15.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.15.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.15.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.15.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.15.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.16.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.16.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.16.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.16.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.16.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.16.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.16.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.16.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.17.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.17.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.17.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.17.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.17.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.17.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.17.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.17.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.18.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.18.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.18.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.18.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.18.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.18.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.18.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.18.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.19.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.19.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.19.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.19.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.19.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.19.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.19.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.19.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.20.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.20.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.20.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.20.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.20.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.20.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.20.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.20.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.21.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.21.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.21.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.21.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.21.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.21.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.21.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.21.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.22.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.22.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.22.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.22.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.22.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.22.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.22.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.22.ln_2.bias', 'cond_stage_model.model.visual.transformer.resblocks.23.attn.in_proj_weight', 'cond_stage_model.model.visual.transformer.resblocks.23.attn.in_proj_bias', 'cond_stage_model.model.visual.transformer.resblocks.23.attn.out_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.23.attn.out_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.23.ln_1.weight', 'cond_stage_model.model.visual.transformer.resblocks.23.ln_1.bias', 'cond_stage_model.model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'cond_stage_model.model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'cond_stage_model.model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'cond_stage_model.model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'cond_stage_model.model.visual.transformer.resblocks.23.ln_2.weight', 'cond_stage_model.model.visual.transformer.resblocks.23.ln_2.bias', 'cond_stage_model.model.visual.ln_post.weight', 'cond_stage_model.model.visual.ln_post.bias', 'cond_stage_model.model.token_embedding.weight', 'cond_stage_model.model.ln_final.weight', 'cond_stage_model.model.ln_final.bias'])\n"
     ]
    }
   ],
   "source": [
    "new_model.load_pre_control(pre_control_cfg=test_cfg.pre_control, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
